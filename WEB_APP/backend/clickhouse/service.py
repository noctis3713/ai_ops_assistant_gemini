#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ClickHouse ç¶²è·¯æµé‡åˆ†ææœå‹™

æä¾›ç¶²è·¯æµé‡çµ±è¨ˆèˆ‡åˆ†æåŠŸèƒ½ï¼š
- æµé‡ç¸½è¦½èˆ‡çµ±è¨ˆ
- Top-N åˆ†æ
- åœ°ç†ä½ç½®èˆ‡ ASN åˆ†æ
- æ™‚é–“åºåˆ—åˆ†æ
"""

import logging
import time
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional, Union

from .client import ClickHouseQueryError, get_clickhouse_client
from .models import (
    ASNStats,
    FlowSummary,
    GeolocationStats,
    HealthCheckResponse,
    InterfaceStats,
    PaginationParams,
    PortStats,
    QueryResponse,
    SortOrder,
    TimeSeriesData,
    TopProtocol,
    TopTalker,
    TrafficAnalysisReport,
)

logger = logging.getLogger(__name__)


class ClickHouseService:
    """ClickHouse ç¶²è·¯æµé‡åˆ†ææœå‹™"""

    def __init__(self):
        """åˆå§‹åŒ–æœå‹™"""
        self.client = get_clickhouse_client()
        logger.info("ClickHouse æœå‹™åˆå§‹åŒ–å®Œæˆ")

    def get_flow_summary(
        self, hours: int = 24, include_details: bool = True
    ) -> Union[FlowSummary, Dict[str, Any]]:
        """å–å¾—æµé‡æ¦‚è¦½çµ±è¨ˆ"""
        start_time = time.time()

        try:
            query = """
            SELECT 
                count() as total_flows,
                sum(Bytes) as total_bytes,
                sum(Packets) as total_packets,
                min(TimeReceived) as time_range_start,
                max(TimeReceived) as time_range_end,
                max(TimeReceived) - min(TimeReceived) as duration_seconds
            FROM flows
            WHERE TimeReceived >= now() - INTERVAL {hours:UInt32} HOUR
            """

            parameters = {"hours": hours}
            result = self.client.execute_query(query, parameters)

            if not result:
                # å¦‚æœæ²’æœ‰è³‡æ–™ï¼Œè¿”å›ç©ºçµ±è¨ˆ
                return FlowSummary(
                    total_flows=0,
                    total_bytes=0,
                    total_packets=0,
                    time_range_start=datetime.now() - timedelta(hours=hours),
                    time_range_end=datetime.now(),
                    duration_seconds=hours * 3600,
                )

            data = result[0]

            # å»ºç«‹ FlowSummary ç‰©ä»¶
            summary = FlowSummary(
                total_flows=data["total_flows"],
                total_bytes=data["total_bytes"],
                total_packets=data["total_packets"],
                time_range_start=data["time_range_start"],
                time_range_end=data["time_range_end"],
                duration_seconds=int(data["duration_seconds"]),
            )

            execution_time = (time.time() - start_time) * 1000
            logger.info(f"æµé‡æ¦‚è¦½æŸ¥è©¢å®Œæˆï¼Œè€—æ™‚ {execution_time:.2f}ms")

            if include_details:
                return {
                    "summary": summary,
                    "execution_time_ms": execution_time,
                    "query_parameters": parameters,
                }

            return summary

        except Exception as e:
            logger.error(f"ç²å–æµé‡æ¦‚è¦½å¤±æ•—: {e}", exc_info=True)
            raise ClickHouseQueryError(f"ç²å–æµé‡æ¦‚è¦½å¤±æ•—: {e}")

    def _get_total_bytes_in_range(self, hours: int) -> int:
        """
        ç²å–æŒ‡å®šæ™‚é–“ç¯„åœå…§çš„ç¸½ä½å…ƒçµ„æ•¸ï¼ˆç”¨æ–¼ç™¾åˆ†æ¯”è¨ˆç®—ï¼‰

        Args:
            hours: çµ±è¨ˆæ™‚é–“ç¯„åœï¼ˆå°æ™‚ï¼‰

        Returns:
            int: ç¸½ä½å…ƒçµ„æ•¸

        Raises:
            ClickHouseQueryError: æŸ¥è©¢å¤±æ•—æ™‚æ‹‹å‡º
        """
        try:
            query = """
            SELECT sum(Bytes) as total_bytes
            FROM flows
            WHERE TimeReceived >= now() - INTERVAL {hours:UInt32} HOUR
            """

            parameters = {"hours": hours}
            result = self.client.execute_query(query, parameters)

            if not result or not result[0]["total_bytes"]:
                logger.warning(f"æŸ¥è©¢æ™‚é–“ç¯„åœå…§ç„¡è³‡æ–™æˆ–ç¸½ä½å…ƒçµ„æ•¸ç‚ºç©º: {hours} å°æ™‚")
                return 0

            return int(result[0]["total_bytes"])

        except Exception as e:
            logger.error(f"ç²å–ç¸½ä½å…ƒçµ„æ•¸å¤±æ•—: {e}", exc_info=True)
            raise ClickHouseQueryError(f"ç²å–ç¸½ä½å…ƒçµ„æ•¸å¤±æ•—: {e}")

    def _get_total_packets_in_range(self, hours: int) -> int:
        """
        ç²å–æŒ‡å®šæ™‚é–“ç¯„åœå…§çš„ç¸½å°åŒ…æ•¸ï¼ˆç”¨æ–¼ç™¾åˆ†æ¯”è¨ˆç®—ï¼‰

        Args:
            hours: çµ±è¨ˆæ™‚é–“ç¯„åœï¼ˆå°æ™‚ï¼‰

        Returns:
            int: ç¸½å°åŒ…æ•¸

        Raises:
            ClickHouseQueryError: æŸ¥è©¢å¤±æ•—æ™‚æ‹‹å‡º
        """
        try:
            query = """
            SELECT sum(Packets) as total_packets
            FROM flows
            WHERE TimeReceived >= now() - INTERVAL {hours:UInt32} HOUR
            """

            parameters = {"hours": hours}
            result = self.client.execute_query(query, parameters)

            if not result or not result[0]["total_packets"]:
                logger.warning(f"æŸ¥è©¢æ™‚é–“ç¯„åœå…§ç„¡è³‡æ–™æˆ–ç¸½å°åŒ…æ•¸ç‚ºç©º: {hours} å°æ™‚")
                return 0

            return int(result[0]["total_packets"])

        except Exception as e:
            logger.error(f"ç²å–ç¸½å°åŒ…æ•¸å¤±æ•—: {e}", exc_info=True)
            raise ClickHouseQueryError(f"ç²å–ç¸½å°åŒ…æ•¸å¤±æ•—: {e}")

    def get_top_talkers(
        self,
        limit: int = 10,
        hours: int = 1,
        by_field: str = "bytes",
        src_or_dst: str = "src",
    ) -> List[TopTalker]:
        """
        ç²å– Top N æµé‡ä¾†æºæˆ–ç›®çš„åœ°

        Args:
            limit: è¿”å›å‰ N ç­†è¨˜éŒ„
            hours: çµ±è¨ˆæ™‚é–“ç¯„åœï¼ˆå°æ™‚ï¼‰
            by_field: æ’åºæ¬„ä½ ('bytes', 'packets', 'flows')
            src_or_dst: çµ±è¨ˆä¾†æºæˆ–ç›®çš„åœ° ('src', 'dst')

        Returns:
            List[TopTalker]: Top N æµé‡ä¾†æº/ç›®çš„åœ°åˆ—è¡¨
        """
        try:
            # ç²å–ç¸½ä½å…ƒçµ„æ•¸ç”¨æ–¼ç™¾åˆ†æ¯”è¨ˆç®—
            total_bytes = self._get_total_bytes_in_range(hours)

            addr_field = "SrcAddr" if src_or_dst == "src" else "DstAddr"

            query = f"""
            SELECT 
                toString({addr_field}) as address,
                sum(Bytes) as bytes,
                sum(Packets) as packets,
                count() as flows
            FROM flows
            WHERE TimeReceived >= now() - INTERVAL {{hours:UInt32}} HOUR
            GROUP BY {addr_field}
            ORDER BY {by_field} DESC
            LIMIT {{limit:UInt32}}
            """

            parameters = {"limit": limit, "hours": hours}
            results = self.client.execute_query(query, parameters)

            # åœ¨ Python å±¤è¨ˆç®—ç™¾åˆ†æ¯”
            top_talkers = []
            for result in results:
                percentage = (
                    (result["bytes"] / total_bytes * 100) if total_bytes > 0 else 0.0
                )
                top_talkers.append(
                    TopTalker(
                        address=result["address"],
                        bytes=result["bytes"],
                        packets=result["packets"],
                        flows=result["flows"],
                        percentage=round(percentage, 2),
                    )
                )

            return top_talkers

        except Exception as e:
            logger.error(f"ç²å– Top Talkers å¤±æ•—: {e}", exc_info=True)
            raise ClickHouseQueryError(f"ç²å– Top Talkers å¤±æ•—: {e}")

    def get_protocol_distribution(
        self, hours: int = 1, limit: int = 10
    ) -> List[TopProtocol]:
        """
        ç²å–å”å®šåˆ†ä½ˆçµ±è¨ˆ

        Args:
            hours: çµ±è¨ˆæ™‚é–“ç¯„åœï¼ˆå°æ™‚ï¼‰
            limit: è¿”å›å‰ N å€‹å”å®š

        Returns:
            List[TopProtocol]: å”å®šçµ±è¨ˆåˆ—è¡¨
        """
        try:
            # ç²å–ç¸½ä½å…ƒçµ„æ•¸ç”¨æ–¼ç™¾åˆ†æ¯”è¨ˆç®—
            total_bytes = self._get_total_bytes_in_range(hours)

            query = """
            SELECT
                Proto as protocol_number,
                '' as protocol_name,
                count() as flows,
                sum(Bytes) as bytes,
                sum(Packets) as packets
            FROM flows
            WHERE TimeReceived >= now() - INTERVAL {hours:UInt32} HOUR
            GROUP BY Proto
            ORDER BY bytes DESC
            LIMIT {limit:UInt32}
            """

            parameters = {"hours": hours, "limit": limit}
            results = self.client.execute_query(query, parameters)

            # åœ¨ Python å±¤è¨ˆç®—ç™¾åˆ†æ¯”
            protocols = []
            for result in results:
                percentage = (
                    (result["bytes"] / total_bytes * 100) if total_bytes > 0 else 0.0
                )
                protocols.append(
                    TopProtocol(
                        protocol_number=result["protocol_number"],
                        protocol_name=result["protocol_name"],
                        flows=result["flows"],
                        bytes=result["bytes"],
                        packets=result["packets"],
                        percentage=round(percentage, 2),
                    )
                )

            return protocols

        except Exception as e:
            logger.error(f"ç²å–å”å®šåˆ†ä½ˆå¤±æ•—: {e}", exc_info=True)
            raise ClickHouseQueryError(f"ç²å–å”å®šåˆ†ä½ˆå¤±æ•—: {e}")

    def get_geolocation_stats(
        self, hours: int = 1, limit: int = 10, by_country_only: bool = False
    ) -> List[GeolocationStats]:
        """
        ç²å–åœ°ç†ä½ç½®æµé‡çµ±è¨ˆï¼ˆåŸå¸‚ç´šæ™ºèƒ½é€€å›ç‰ˆï¼‰

        Args:
            hours: çµ±è¨ˆæ™‚é–“ç¯„åœï¼ˆå°æ™‚ï¼‰
            limit: è¿”å›å‰ N å€‹ä½ç½®  
            by_country_only: æ˜¯å¦åªæŒ‰åœ‹å®¶çµ±è¨ˆï¼ˆé è¨­Falseï¼Œå„ªå…ˆä½¿ç”¨åŸå¸‚ç´šè³‡æ–™ï¼‰

        Returns:
            List[GeolocationStats]: åœ°ç†ä½ç½®çµ±è¨ˆåˆ—è¡¨
        """
        try:
            # ç²å–ç¸½ä½å…ƒçµ„æ•¸ç”¨æ–¼ç™¾åˆ†æ¯”è¨ˆç®—
            query_total = """
            SELECT sum(Bytes) as total_bytes
            FROM flows
            WHERE TimeReceived >= now() - INTERVAL {hours:UInt32} HOUR
            """
            parameters = {"hours": hours, "limit": limit}
            total_result = self.client.execute_query(query_total, parameters)
            total_bytes = (
                int(total_result[0]["total_bytes"])
                if total_result and total_result[0]["total_bytes"]
                else 0
            )

            if by_country_only:
                # åªé¡¯ç¤ºåœ‹å®¶ç´šè³‡æ–™
                query = """
                SELECT
                    SrcCountry as country,
                    NULL as city,
                    NULL as state,
                    'country' as granularity,
                    count() as flows,
                    sum(Bytes) as bytes,
                    sum(Packets) as packets,
                    uniq(SrcAddr) as unique_ips
                FROM flows
                WHERE TimeReceived >= now() - INTERVAL {hours:UInt32} HOUR
                  AND SrcCountry <> ''
                GROUP BY SrcCountry
                ORDER BY bytes DESC
                LIMIT {limit:UInt32}
                """
            else:
                # æ™ºèƒ½åœ°ç†åˆ†æï¼šåŸå¸‚å„ªå…ˆï¼Œé€€å›åœ‹å®¶
                query = """
                SELECT 
                    SrcCountry as country,
                    CASE WHEN SrcGeoCity <> '' THEN SrcGeoCity ELSE NULL END as city,
                    CASE WHEN SrcGeoState <> '' THEN SrcGeoState ELSE NULL END as state,
                    CASE 
                        WHEN SrcGeoCity <> '' THEN 'city'
                        WHEN SrcCountry <> '' THEN 'country'
                        ELSE 'unknown'
                    END as granularity,
                    COUNT(*) as flows,
                    SUM(Bytes) as bytes,
                    SUM(Packets) as packets,
                    uniq(SrcAddr) as unique_ips
                FROM flows
                WHERE TimeReceived >= now() - INTERVAL {hours:UInt32} HOUR
                GROUP BY country, city, state, granularity
                ORDER BY bytes DESC
                LIMIT {limit:UInt32}
                """

            results = self.client.execute_query(query, parameters)

            # åœ¨ Python å±¤è¨ˆç®—ç™¾åˆ†æ¯”å’Œå»ºç«‹ç‰©ä»¶
            geo_stats = []
            for result in results:
                percentage = (
                    (result["bytes"] / total_bytes * 100) if total_bytes > 0 else 0.0
                )
                geo_stats.append(
                    GeolocationStats(
                        country=result["country"] or "",
                        city=result["city"],
                        state=result["state"],
                        granularity=result.get("granularity", "unknown"),
                        flows=result["flows"],
                        bytes=result["bytes"],
                        packets=result["packets"],
                        unique_ips=result.get("unique_ips", 0),
                        percentage=round(percentage, 2),
                    )
                )

            return geo_stats

        except Exception as e:
            logger.error(f"ç²å–åœ°ç†ä½ç½®çµ±è¨ˆå¤±æ•—: {e}", exc_info=True)
            raise ClickHouseQueryError(f"ç²å–åœ°ç†ä½ç½®çµ±è¨ˆå¤±æ•—: {e}")

    def get_asn_analysis(
        self, hours: int = 1, limit: int = 10, src_or_dst: str = "src"
    ) -> List[ASNStats]:
        """
        ç²å– ASN è‡ªæ²»ç³»çµ±åˆ†æ

        Args:
            hours: çµ±è¨ˆæ™‚é–“ç¯„åœï¼ˆå°æ™‚ï¼‰
            limit: è¿”å›å‰ N å€‹ ASN
            src_or_dst: åˆ†æä¾†æºæˆ–ç›®çš„ ASN ('src', 'dst')

        Returns:
            List[ASNStats]: ASN çµ±è¨ˆåˆ—è¡¨
        """
        try:
            asn_field = "SrcAS" if src_or_dst == "src" else "DstAS"
            addr_field = "SrcAddr" if src_or_dst == "src" else "DstAddr"

            # ç²å–æœ‰ ASN è³‡æ–™çš„ç¸½ä½å…ƒçµ„æ•¸ç”¨æ–¼ç™¾åˆ†æ¯”è¨ˆç®—
            query_total = f"""
            SELECT sum(Bytes) as total_bytes
            FROM flows
            WHERE TimeReceived >= now() - INTERVAL {{hours:UInt32}} HOUR
              AND {asn_field} != 0
            """
            parameters = {"hours": hours}
            total_result = self.client.execute_query(query_total, parameters)
            total_bytes = (
                int(total_result[0]["total_bytes"])
                if total_result and total_result[0]["total_bytes"]
                else 0
            )

            query = f"""
            SELECT
                {asn_field} as asn,
                '' as asn_name,
                count() as flows,
                sum(Bytes) as bytes,
                sum(Packets) as packets,
                uniq({addr_field}) as unique_ips
            FROM flows
            WHERE TimeReceived >= now() - INTERVAL {{hours:UInt32}} HOUR
              AND {asn_field} != 0
            GROUP BY {asn_field}
            ORDER BY bytes DESC
            LIMIT {{limit:UInt32}}
            """

            parameters = {"hours": hours, "limit": limit}
            results = self.client.execute_query(query, parameters)

            # åœ¨ Python å±¤è¨ˆç®—ç™¾åˆ†æ¯”
            asn_stats = []
            for result in results:
                percentage = (
                    (result["bytes"] / total_bytes * 100) if total_bytes > 0 else 0.0
                )
                asn_stats.append(
                    ASNStats(
                        asn=result["asn"],
                        asn_name=result["asn_name"],
                        flows=result["flows"],
                        bytes=result["bytes"],
                        packets=result["packets"],
                        percentage=round(percentage, 2),
                        unique_ips=result["unique_ips"],
                    )
                )

            return asn_stats

        except Exception as e:
            logger.error(f"ç²å– ASN åˆ†æå¤±æ•—: {e}", exc_info=True)
            raise ClickHouseQueryError(f"ç²å– ASN åˆ†æå¤±æ•—: {e}")

    def get_time_series_data(
        self, hours: int = 24, interval_minutes: int = 5
    ) -> List[TimeSeriesData]:
        """
        ç²å–æ™‚é–“åºåˆ—æµé‡è³‡æ–™

        Args:
            hours: çµ±è¨ˆæ™‚é–“ç¯„åœï¼ˆå°æ™‚ï¼‰
            interval_minutes: æ™‚é–“é–“éš”ï¼ˆåˆ†é˜ï¼‰

        Returns:
            List[TimeSeriesData]: æ™‚é–“åºåˆ—è³‡æ–™åˆ—è¡¨
        """
        try:
            query = """
            SELECT
                toStartOfInterval(TimeReceived, INTERVAL {interval:UInt32} MINUTE) as timestamp,
                count() as flows,
                sum(Bytes) as bytes,
                sum(Packets) as packets,
                uniq(SrcAddr) as unique_src_ips,
                uniq(DstAddr) as unique_dst_ips
            FROM flows
            WHERE TimeReceived >= now() - INTERVAL {hours:UInt32} HOUR
            GROUP BY timestamp
            ORDER BY timestamp
            """

            parameters = {"hours": hours, "interval": interval_minutes}
            results = self.client.execute_query(query, parameters)

            return [TimeSeriesData(**result) for result in results]

        except Exception as e:
            logger.error(f"ç²å–æ™‚é–“åºåˆ—è³‡æ–™å¤±æ•—: {e}", exc_info=True)
            raise ClickHouseQueryError(f"ç²å–æ™‚é–“åºåˆ—è³‡æ–™å¤±æ•—: {e}")



    def get_health_status(self) -> HealthCheckResponse:
        """
        ç²å– ClickHouse å¥åº·ç‹€æ…‹

        Returns:
            HealthCheckResponse: å¥åº·æª¢æŸ¥å›æ‡‰
        """
        try:
            health_info = self.client.test_connection()
            return HealthCheckResponse(**health_info)

        except Exception as e:
            logger.error(f"å¥åº·æª¢æŸ¥å¤±æ•—: {e}", exc_info=True)
            return HealthCheckResponse(
                status="error", database="akvorado", error=str(e)
            )


    def get_traffic_analysis(self, days: int = 3, device: Optional[str] = None) -> TrafficAnalysisReport:
        """åŸ·è¡Œç¶²è·¯æµé‡åˆ†æ"""
        start_time = time.time()
        
        try:
            parameters = {"days": days, "device": device or ""}
            
            # 1. æµé‡ç¸½è¦½çµ±è¨ˆ
            overview_results = self.client.execute_query("""
            SELECT 
                COUNT(*) as total_flows,
                SUM(Bytes) as total_bytes,
                SUM(Packets) as total_packets,
                min(TimeReceived) as time_range_start,
                max(TimeReceived) as time_range_end,
                max(TimeReceived) - min(TimeReceived) as duration_seconds
            FROM flows
            WHERE TimeReceived >= now() - INTERVAL {days:UInt32} DAY
              AND ({device:String} = '' OR ExporterName = {device:String})
            """, parameters)
            
            # åŸ·è¡ŒåŸºæœ¬æŸ¥è©¢ä¸¦è¨ˆç®—ç™¾åˆ†æ¯”ï¼ˆå…§å­˜å„ªåŒ–ï¼‰
            logger.info(f"é–‹å§‹åŸ·è¡Œæµé‡åˆ†ææŸ¥è©¢ - {days} å¤©ç¯„åœ")
            
            # ç²å–ç¸½æµé‡ç”¨æ–¼ç™¾åˆ†æ¯”è¨ˆç®—
            if overview_results and overview_results[0]:
                total_bytes = overview_results[0]["total_bytes"] or 0
            else:
                total_bytes = 0
            
            # 2. Top 10 æµé‡ä¾†æº
            top_sources_results = self.client.execute_query("""
            SELECT 
                IPv6NumToString(SrcAddr) as address,
                COUNT(*) as flows,
                SUM(Bytes) as bytes,
                SUM(Packets) as packets
            FROM flows
            WHERE TimeReceived >= now() - INTERVAL {days:UInt32} DAY
              AND ({device:String} = '' OR ExporterName = {device:String})
            GROUP BY SrcAddr
            ORDER BY bytes DESC
            LIMIT 10
            """, parameters)
            
            # 3. Top 10 æµé‡ç›®çš„åœ°
            top_destinations_results = self.client.execute_query("""
            SELECT 
                IPv6NumToString(DstAddr) as address,
                COUNT(*) as flows,
                SUM(Bytes) as bytes,
                SUM(Packets) as packets
            FROM flows
            WHERE TimeReceived >= now() - INTERVAL {days:UInt32} DAY
              AND ({device:String} = '' OR ExporterName = {device:String})
            GROUP BY DstAddr
            ORDER BY bytes DESC
            LIMIT 10
            """, parameters)
            
            # 4. Top 10 å”è­°åŠæ‡‰ç”¨ç¨‹å¼åˆ†å¸ƒ
            protocols_results = self.client.execute_query("""
            SELECT 
                Proto as protocol_number,
                CASE 
                    WHEN Proto = 1 THEN 'ICMP'
                    WHEN Proto = 6 THEN 'TCP'
                    WHEN Proto = 17 THEN 'UDP'
                    WHEN Proto = 47 THEN 'GRE'
                    WHEN Proto = 50 THEN 'ESP'
                    WHEN Proto = 51 THEN 'AH'
                    WHEN Proto = 89 THEN 'OSPF'
                    WHEN Proto = 132 THEN 'SCTP'
                    WHEN Proto = 41 THEN 'IPv6-in-IPv4'
                    WHEN Proto = 2 THEN 'IGMP'
                    ELSE concat('Protocol-', toString(Proto))
                END as protocol_name,
                COUNT(*) as flows,
                SUM(Bytes) as bytes,
                SUM(Packets) as packets
            FROM flows
            WHERE TimeReceived >= now() - INTERVAL {days:UInt32} DAY
              AND ({device:String} = '' OR ExporterName = {device:String})
            GROUP BY Proto
            ORDER BY bytes DESC
            LIMIT 10
            """, parameters)
            
            # æ¯æ—¥è¶¨å‹¢
            daily_trends_results = self.client.execute_query("""
            SELECT 
                toDate(TimeReceived) as date,
                COUNT(*) as flows,
                SUM(Bytes) as bytes,
                SUM(Packets) as packets,
                round(SUM(Bytes) / 1024 / 1024, 2) as bytes_mb
            FROM flows
            WHERE TimeReceived >= now() - INTERVAL {days:UInt32} DAY
              AND ({device:String} = '' OR ExporterName = {device:String})
            GROUP BY date
            ORDER BY date
            """, parameters)
            
            # 24å°æ™‚æ¨¡å¼
            hourly_patterns_results = self.client.execute_query("""
            SELECT 
                toHour(TimeReceived) as hour,
                COUNT(*) as flows,
                SUM(Bytes) as bytes,
                SUM(Packets) as packets,
                round(SUM(Bytes) / 1024 / 1024, 2) as bytes_mb
            FROM flows
            WHERE TimeReceived >= now() - INTERVAL {days:UInt32} DAY
              AND ({device:String} = '' OR ExporterName = {device:String})
            GROUP BY hour
            ORDER BY hour
            """, parameters)
            
            # åœ°ç†ä½ç½®åˆ†æ
            geo_results = self.client.execute_query("""
            WITH country_city_check AS (
                SELECT 
                    SrcCountry,
                    countIf(SrcGeoCity <> '') as has_city_data,
                    count(*) as total_flows
                FROM flows
                WHERE TimeReceived >= now() - INTERVAL {days:UInt32} DAY
                  AND SrcCountry <> ''
                  AND ({device:String} = '' OR ExporterName = {device:String})
                GROUP BY SrcCountry
            ),
            location_data AS (
                SELECT 
                    CASE 
                        WHEN SrcCountry <> '' THEN SrcCountry
                        ELSE 'Unknown'
                    END as country,
                    CASE 
                        WHEN SrcGeoCity <> '' THEN SrcGeoCity
                        ELSE NULL
                    END as city,
                    CASE 
                        WHEN SrcGeoState <> '' THEN SrcGeoState
                        ELSE NULL
                    END as state,
                    CASE 
                        WHEN SrcGeoCity <> '' THEN 'city'
                        WHEN SrcGeoState <> '' AND SrcGeoCity = '' THEN 'state'
                        WHEN SrcCountry <> '' THEN 'country'
                        ELSE 'unknown'
                    END as granularity,
                    COUNT(*) as flows,
                    SUM(Bytes) as bytes,
                    SUM(Packets) as packets,
                    uniq(SrcAddr) as unique_ips
                FROM flows
                WHERE TimeReceived >= now() - INTERVAL {days:UInt32} DAY
                  AND ({device:String} = '' OR ExporterName = {device:String})
                GROUP BY country, city, state, granularity
            )
            SELECT 
                country,
                city,
                state,
                granularity,
                flows,
                bytes,
                packets,
                unique_ips
            FROM location_data l
            WHERE 
                -- é¡¯ç¤ºåŸå¸‚ç´šè³‡æ–™
                granularity = 'city'
                OR granularity = 'state' 
                OR granularity = 'unknown'
                -- åªæœ‰åœ¨è©²åœ‹å®¶å®Œå…¨æ²’æœ‰åŸå¸‚è³‡æ–™æ™‚æ‰é¡¯ç¤ºåœ‹å®¶ç´š
                OR (granularity = 'country' AND country NOT IN (
                    SELECT DISTINCT country 
                    FROM location_data 
                    WHERE granularity IN ('city', 'state')
                ))
            ORDER BY bytes DESC
            LIMIT 15
            """, parameters)
            
            # Top 10 ASN åˆ†æï¼ˆå«çµ„ç¹”åç¨±ï¼‰
            asn_results = self.client.execute_query("""
            SELECT 
                SrcAS as asn,
                dictGet('asns', 'name', SrcAS) as asn_name,
                COUNT(*) as flows,
                SUM(Bytes) as bytes,
                SUM(Packets) as packets,
                uniq(SrcAddr) as unique_ips
            FROM flows
            WHERE TimeReceived >= now() - INTERVAL {days:UInt32} DAY AND SrcAS != 0
              AND ({device:String} = '' OR ExporterName = {device:String})
            GROUP BY SrcAS
            ORDER BY bytes DESC
            LIMIT 10
            """, parameters)
            
            # è™•ç†æŸ¥è©¢çµæœä¸¦è¨ˆç®—ç™¾åˆ†æ¯”
            execution_time = (time.time() - start_time) * 1000
            
            return self._build_optimized_report(
                days, total_bytes,
                overview_results, top_sources_results, top_destinations_results,
                protocols_results, 
                daily_trends_results, hourly_patterns_results,
                geo_results, asn_results,
                execution_time
            )
            
        except Exception as e:
            logger.error(f"æµé‡åˆ†æå¤±æ•—: {e}", exc_info=True)
            raise ClickHouseQueryError(f"æµé‡åˆ†æåŸ·è¡Œå¤±æ•—: {e}")
    
    def _build_optimized_report(self, days, total_bytes, overview_results, 
                              top_sources_results, top_destinations_results,
                              protocols_results,
                              daily_trends_results, hourly_patterns_results,
                              geo_results, asn_results,
                              execution_time) -> TrafficAnalysisReport:
        """æ§‹å»ºæµé‡åˆ†æå ±å‘Š"""
        
        # è™•ç†ç¸½è¦½è³‡æ–™
        overview_data = None
        if overview_results:
            row = overview_results[0]
            overview_data = FlowSummary(
                total_flows=row["total_flows"],
                total_bytes=row["total_bytes"],
                total_packets=row["total_packets"],
                time_range_start=row["time_range_start"],
                time_range_end=row["time_range_end"],
                duration_seconds=row["duration_seconds"]
            )
        
        # è¨ˆç®—ç™¾åˆ†æ¯”
        def calculate_percentage(bytes_value, total):
            return round((bytes_value / total * 100) if total > 0 else 0, 2)
            
        # è™•ç†å„é …çµæœ
        top_sources = [TopTalker(
            address=row["address"], flows=row["flows"], bytes=row["bytes"],
            packets=row["packets"], percentage=calculate_percentage(row["bytes"], total_bytes)
        ) for row in top_sources_results]
        
        top_destinations = [TopTalker(
            address=row["address"], flows=row["flows"], bytes=row["bytes"],
            packets=row["packets"], percentage=calculate_percentage(row["bytes"], total_bytes)
        ) for row in top_destinations_results]
        
        protocol_distribution = [TopProtocol(
            protocol_number=row["protocol_number"], protocol_name=row["protocol_name"],
            flows=row["flows"], bytes=row["bytes"], packets=row["packets"],
            percentage=calculate_percentage(row["bytes"], total_bytes)
        ) for row in protocols_results]
        
        # åœ°ç†ä½ç½®åˆ†æ
        geographic_distribution = [GeolocationStats(
            country=row["country"], 
            city=row.get("city", None),
            state=row.get("state", None),
            granularity=row.get("granularity", "unknown"),
            flows=row["flows"], 
            bytes=row["bytes"], 
            packets=row["packets"],
            unique_ips=row.get("unique_ips", 0),
            percentage=calculate_percentage(row["bytes"], total_bytes)
        ) for row in geo_results]
        
        # ASN åˆ†æ
        asn_analysis = [ASNStats(
            asn=row["asn"], asn_name=row.get("asn_name", ""),
            flows=row["flows"], bytes=row["bytes"], packets=row["packets"],
            percentage=calculate_percentage(row["bytes"], total_bytes),
            unique_ips=row["unique_ips"]
        ) for row in asn_results]
        
        # æ™‚é–“è¶¨å‹¢
        daily_trends = [{
            "date": row["date"].strftime("%Y-%m-%d"),
            "flows": row["flows"], "bytes": row["bytes"],
            "packets": row["packets"], "bytes_mb": row["bytes_mb"]
        } for row in daily_trends_results]
        
        hourly_patterns = [{
            "hour": row["hour"], "flows": row["flows"], 
            "bytes": row["bytes"], "packets": row["packets"], "bytes_mb": row["bytes_mb"]
        } for row in hourly_patterns_results]
        
        # ç”Ÿæˆåˆ†ææ‘˜è¦
        key_findings = self._generate_enhanced_key_findings(
            overview_data, top_sources, top_destinations, protocol_distribution,
            geographic_distribution, asn_analysis
        )
        anomalies = self._detect_enhanced_anomalies(
            overview_data, top_sources, daily_trends
        )
        
        return TrafficAnalysisReport(
            period_days=days,
            time_range={
                "start": overview_data.time_range_start if overview_data else datetime.now() - timedelta(days=days),
                "end": overview_data.time_range_end if overview_data else datetime.now()
            },
            overview=overview_data or FlowSummary(
                total_flows=0, total_bytes=0, total_packets=0,
                time_range_start=datetime.now() - timedelta(days=days),
                time_range_end=datetime.now(), duration_seconds=days * 86400
            ),
            top_sources=top_sources,
            top_destinations=top_destinations,
            protocol_distribution=protocol_distribution,
            geographic_distribution=geographic_distribution,
            asn_analysis=asn_analysis,
            daily_trends=daily_trends,
            hourly_patterns=hourly_patterns,
            key_findings=key_findings,
            anomalies=anomalies,
            query_time_ms=execution_time
        )
            

    def _generate_enhanced_key_findings(
        self, 
        overview: Optional[FlowSummary], 
        top_sources: List[TopTalker],
        top_destinations: List[TopTalker],
        protocols: List[TopProtocol],
        geographic_distribution: List[GeolocationStats],
        asn_analysis: List[ASNStats]
    ) -> List[str]:
        """ç”Ÿæˆé—œéµç™¼ç¾"""
        findings = []
        
        if not overview or not top_sources:
            return ["ç„¡è¶³å¤ è³‡æ–™é€²è¡Œåˆ†æ"]
            
        # 1. åŸºæœ¬æµé‡çµ±è¨ˆ
        total_gb = overview.total_bytes / (1024**3)
        total_mbps = (overview.total_bytes * 8) / (1024**2) / overview.duration_seconds if overview.duration_seconds > 0 else 0
        findings.append(f"ç¸½æµé‡: {overview.total_flows:,} ç­†è¨˜éŒ„ï¼Œ{total_gb:.2f} GBï¼Œå¹³å‡ {total_mbps:.1f} Mbps")
        
        # 2. æµé‡ä¾†æºåˆ†æ
        if top_sources:
            top_source = top_sources[0]
            findings.append(f"æœ€å¤§æµé‡ä¾†æº: {top_source.address} ({top_source.percentage:.1f}%ï¼Œ{top_source.bytes/(1024**2):.0f} MB)")
            
        if top_destinations:
            top_dest = top_destinations[0]
            findings.append(f"æœ€å¤§æµé‡ç›®çš„: {top_dest.address} ({top_dest.percentage:.1f}%ï¼Œ{top_dest.bytes/(1024**2):.0f} MB)")
            
        # 3. å”è­°åˆ†å¸ƒåˆ†æ
        if protocols:
            tcp_pct = next((p.percentage for p in protocols if p.protocol_name == "TCP"), 0)
            udp_pct = next((p.percentage for p in protocols if p.protocol_name == "UDP"), 0)
            icmp_pct = next((p.percentage for p in protocols if p.protocol_name == "ICMP"), 0)
            findings.append(f"å”è­°åˆ†å¸ƒ: TCP {tcp_pct:.1f}%, UDP {udp_pct:.1f}%, ICMP {icmp_pct:.1f}%")
            
        # 4. åœ°ç†ä½ç½®åˆ†æ
        if geographic_distribution:
            top_country = geographic_distribution[0]
            total_countries = len(geographic_distribution)
            findings.append(f"åœ°ç†åˆ†å¸ƒ: ä¾†è‡ª {total_countries} å€‹åœ‹å®¶/åœ°å€ï¼Œä¸»è¦ä¾†æº {top_country.country} ({top_country.percentage:.1f}%)")
            
        # 5. ASN åˆ†æ
        if asn_analysis:
            top_asn = asn_analysis[0]
            total_asns = len(asn_analysis)
            findings.append(f"ASN åˆ†æ: {total_asns} å€‹è‡ªæ²»ç³»çµ±ï¼Œä¸»è¦ä¾†æº AS{top_asn.asn} ({top_asn.percentage:.1f}%ï¼Œ{top_asn.unique_ips} å€‹ IP)")
            
        # 6. æµé‡æ¨¡å¼
        avg_bytes_per_flow = overview.total_bytes / overview.total_flows if overview.total_flows > 0 else 0
        avg_packets_per_flow = overview.total_packets / overview.total_flows if overview.total_flows > 0 else 0
        findings.append(f"æµé‡æ¨¡å¼: å¹³å‡æ¯æµé‡ {avg_bytes_per_flow:.0f} ä½å…ƒçµ„ï¼Œ{avg_packets_per_flow:.1f} å°åŒ…")
            
        return findings[:15]  # é™åˆ¶æœ€å¤š15å€‹ç™¼ç¾
    
    def _detect_enhanced_anomalies(
        self,
        overview: Optional[FlowSummary],
        top_sources: List[TopTalker],
        daily_trends: List[Dict]
    ) -> List[str]:
        """æª¢æ¸¬ç•°å¸¸æµé‡"""
        anomalies = []
        
        # 1. æµé‡é›†ä¸­åº¦ç•°å¸¸
        if top_sources:
            if top_sources[0].percentage > 50:
                anomalies.append(f"âš ï¸ æµé‡é«˜åº¦é›†ä¸­: {top_sources[0].address} å  {top_sources[0].percentage:.1f}% (è¶…é50%)")
            
            # æª¢æŸ¥å‰3åæµé‡ä¾†æºæ˜¯å¦ä½”æ¯”éé«˜
            if len(top_sources) >= 3:
                top3_total = sum(source.percentage for source in top_sources[:3])
                if top3_total > 80:
                    anomalies.append(f"âš ï¸ å‰3åä¾†æºæµé‡é›†ä¸­: ç¸½è¨ˆ {top3_total:.1f}% (è¶…é80%)")
                    
        # 2. æ¯æ—¥æµé‡ç•°å¸¸
        if len(daily_trends) > 1:
            bytes_values = [day["bytes"] for day in daily_trends]
            if bytes_values:
                avg_bytes = sum(bytes_values) / len(bytes_values)
                max_bytes = max(bytes_values)
                min_bytes = min(bytes_values)
                
                # æª¢æ¸¬ç•°å¸¸é«˜æµé‡æ—¥
                for day in daily_trends:
                    if day["bytes"] > avg_bytes * 3:
                        anomalies.append(f"ğŸ”´ ç•°å¸¸é«˜æµé‡æ—¥: {day['date']} ({day['bytes_mb']:.0f} MBï¼Œè¶…éå¹³å‡å€¼3å€)")
                        
                # æª¢æ¸¬ç•°å¸¸ä½æµé‡æ—¥  
                for day in daily_trends:
                    if day["bytes"] < avg_bytes * 0.1 and day["bytes"] > 0:
                        anomalies.append(f"ğŸ”µ ç•°å¸¸ä½æµé‡æ—¥: {day['date']} ({day['bytes_mb']:.0f} MBï¼Œä½æ–¼å¹³å‡å€¼90%)")
                        
                # æª¢æ¸¬æµé‡è®ŠåŒ–ç•°å¸¸
                if max_bytes > min_bytes * 10 and min_bytes > 0:
                    anomalies.append(f"âš¡ æµé‡æ³¢å‹•ç•°å¸¸: æœ€é«˜èˆ‡æœ€ä½æ—¥æµé‡æ¯”ç‡è¶…é 10:1")
                    
        # 3. æµé‡è¦æ¨¡ç•°å¸¸
        if overview:
            hourly_avg_flows = overview.total_flows / (overview.duration_seconds / 3600) if overview.duration_seconds > 0 else 0
            if hourly_avg_flows > 1000000:  # æ¯å°æ™‚è¶…é100è¬æ¢æµé‡è¨˜éŒ„
                anomalies.append(f"ğŸ“Š è¶…é«˜æµé‡å¯†åº¦: æ¯å°æ™‚å¹³å‡ {hourly_avg_flows:.0f} æ¢è¨˜éŒ„")
                
        # 5. å”è­°ç•°å¸¸ï¼ˆå¦‚æœæœ‰çš„è©±ï¼‰
        # é€™è£¡å¯ä»¥æ·»åŠ æ›´å¤šå”è­°ç›¸é—œçš„ç•°å¸¸æª¢æ¸¬
        
        return anomalies[:10]  # é™åˆ¶æœ€å¤š10å€‹ç•°å¸¸

    def _generate_key_findings(
        self, 
        overview: Optional[FlowSummary], 
        top_sources: List[TopTalker],
        top_destinations: List[TopTalker],
        protocols: List[TopProtocol]
    ) -> List[str]:
        """ç”Ÿæˆé—œéµç™¼ç¾ï¼ˆå…¼å®¹èˆŠæ–¹æ³•ï¼‰"""
        return self._generate_enhanced_key_findings(
            overview, top_sources, top_destinations, protocols, [], []
        )
    
    def _detect_anomalies(
        self,
        overview: Optional[FlowSummary],
        top_sources: List[TopTalker],
        daily_trends: List[Dict]
    ) -> List[str]:
        """æª¢æ¸¬ç•°å¸¸æµé‡ï¼ˆå…¼å®¹èˆŠæ–¹æ³•ï¼‰"""
        return self._detect_enhanced_anomalies(overview, top_sources, daily_trends)


# å…¨åŸŸæœå‹™å¯¦ä¾‹
_clickhouse_service: Optional[ClickHouseService] = None


def get_clickhouse_service() -> ClickHouseService:
    """
    ç²å–å…¨åŸŸ ClickHouse æœå‹™å¯¦ä¾‹

    Returns:
        ClickHouseService: ClickHouse æœå‹™å¯¦ä¾‹
    """
    global _clickhouse_service
    if _clickhouse_service is None:
        _clickhouse_service = ClickHouseService()
    return _clickhouse_service
