#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AI æ™ºèƒ½åˆ†ææœå‹™æ¨¡çµ„

æä¾›ç¶²è·¯è¨­å‚™çš„ AI æ™ºèƒ½åˆ†æå’ŒæŸ¥è©¢åŠŸèƒ½ï¼š
- Google Gemini å’Œ Anthropic Claude é›™ AI å¼•æ“æ”¯æ´
- ç¶²è·¯è¨­å‚™æŒ‡ä»¤åŸ·è¡Œå’Œçµæœåˆ†æ
- è‡ªå‹•åŒ–çš„è¼¸å‡ºæ‘˜è¦å’Œé•·æ–‡æœ¬è™•ç†
- ReAct ä»£ç†å’Œå·¥å…·ä¸²æ¥
"""

import asyncio
import logging
import os
import time
import uuid
from functools import lru_cache
from typing import Any, Dict, List, Optional, Tuple

# AI æœå‹™ç›¸é—œå°å…¥
try:
    import warnings

    from langchain import hub
    from langchain.agents import AgentExecutor, create_react_agent
    from langchain_anthropic import ChatAnthropic
    from langchain_core.callbacks import UsageMetadataCallbackHandler
    from langchain_core.output_parsers import PydanticOutputParser
    from langchain_core.prompts import PromptTemplate
    from langchain_core.tools import Tool
    from langchain_google_genai import ChatGoogleGenerativeAI

    warnings.filterwarnings("ignore")
    AI_AVAILABLE = True
except ImportError:
    AI_AVAILABLE = False


from common import NetworkAnalysisResponse
from exceptions import ExternalServiceError, ai_error
from network import batch_command_wrapper, set_device_scope_restriction
from prompt_manager import get_prompt_manager
from settings import settings

logger = logging.getLogger(__name__)


# AI æœå‹™æ¨¡çµ„ï¼Œä½¿ç”¨ PromptManager é€²è¡Œæç¤ºè©ç®¡ç†


def _get_few_shot_examples():
    """ç²å– AI æ€è€ƒéˆç¤ºç¯„ç¯„ä¾‹

    é€éæç¤ºè©ç®¡ç†å™¨è¼‰å…¥é å®šç¾©çš„ ReAct ç¯„ä¾‹ï¼Œ
    å¹«åŠ© AI ç†è§£å¦‚ä½•é€æ­¥åˆ†æç¶²è·¯å•é¡Œã€‚
    """
    try:
        # ç²å– PromptManager å¯¦ä¾‹
        prompt_manager = get_prompt_manager()

        # ä½¿ç”¨ PromptManager æ¸²æŸ“æ€è€ƒéˆç¯„ä¾‹
        return prompt_manager.render_react_examples()
    except Exception as e:
        logger.error(f"ç²å–æ€è€ƒéˆç¯„ä¾‹å¤±æ•—: {e}")
        # è¿”å›ç©ºå­—ä¸²ä½œç‚ºå¾Œå‚™ï¼Œé¿å…å®Œå…¨å¤±æ•—
        return ""


def get_ai_logger():
    """å»ºç«‹ AI æ¨¡çµ„å°ˆç”¨çš„æ—¥èªŒè¨˜éŒ„å™¨

    ç¨ç«‹çš„æ—¥èªŒé€šé“ï¼Œç”¨æ–¼è¿½è¹¤ AI æœå‹™çš„åŸ·è¡Œç‹€æ…‹ã€‚
    åªè¼¸å‡ºåˆ°æ§åˆ¶å°ï¼Œä¸å¯«å…¥æª”æ¡ˆã€‚
    """
    import logging

    ai_logger = logging.getLogger("ai_service")
    # ä½¿ç”¨ DEBUG ç´šåˆ¥ä»¥é¡¯ç¤ºæ›´å¤šè¨ºæ–·è³‡è¨Š
    ai_logger.setLevel(logging.DEBUG)

    # ç¢ºä¿ ai_logger ä¸æœƒé‡è¤‡æ·»åŠ  handler
    if not ai_logger.handlers:
        # å‰µå»ºæ§åˆ¶å°è™•ç†å™¨ï¼Œç”¨æ–¼ token ä½¿ç”¨é‡çš„ç‰¹æ®Šé¡¯ç¤º
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.DEBUG)

        # å‰µå»ºè‡ªå®šç¾©æ ¼å¼å™¨ï¼Œçªå‡ºé¡¯ç¤º TOKEN_USAGE ä¿¡æ¯
        class TokenUsageFormatter(logging.Formatter):
            def format(self, record):
                if "TOKEN_USAGE_ESTIMATED:" in record.getMessage():
                    # ç‚º token ä¼°ç®—å€¼å‰µå»ºç‰¹æ®Šæ ¼å¼
                    return f"ğŸ“Š [TOKEN-ä¼°ç®—] {record.getMessage().replace('TOKEN_USAGE_ESTIMATED: ', '')}"
                elif "TOKEN_USAGE:" in record.getMessage():
                    # ç‚º token ä½¿ç”¨é‡å‰µå»ºç‰¹æ®Šæ ¼å¼
                    return f"ğŸ’° [TOKEN] {record.getMessage()}"
                else:
                    # ä½¿ç”¨çµ±ä¸€æ¨™æº–æ ¼å¼
                    return f"ğŸ¤– [AI] {record.levelname}: {record.getMessage()}"

        formatter = TokenUsageFormatter()
        console_handler.setFormatter(formatter)
        ai_logger.addHandler(console_handler)

        # é˜²æ­¢æ—¥èªŒæ¶ˆæ¯å‚³æ’­åˆ°æ ¹æ—¥èªŒè¨˜éŒ„å™¨ï¼ˆé¿å…é‡è¤‡ï¼‰
        ai_logger.propagate = False

    return ai_logger


ai_logger = get_ai_logger()


class OutputSummarizer:
    """è¨­å‚™è¼¸å‡ºå…§å®¹çš„æ™ºèƒ½æ‘˜è¦æœå‹™

    ç•¶ç¶²è·¯è¨­å‚™è¼¸å‡ºéé•·æ™‚ï¼Œä½¿ç”¨ AI é€²è¡Œæ‘˜è¦å’Œç¸½çµï¼Œ
    ä¿ç•™é—œéµè³‡è¨ŠåŒæ™‚æ¸›å°‘å†—é¤˜å…§å®¹ã€‚
    """

    def __init__(self, ai_provider: str = None, model_name: str = None):
        self.ai_provider = ai_provider or settings.AI_PROVIDER
        self.max_tokens = 2048
        self.llm = None

        if self.ai_provider == "claude":
            self.model_name = model_name or settings.CLAUDE_MODEL
        else:
            self.model_name = model_name or settings.GEMINI_MODEL

        self._initialize_ai_service()

    def _initialize_ai_service(self):
        """æ ¹æ“šé…ç½®åˆå§‹åŒ–å°æ‡‰çš„ AI æœå‹™

        æ ¹æ“š ai_provider è¨­å®šé¸æ“‡ Gemini æˆ– Claude é€²è¡Œåˆå§‹åŒ–ã€‚
        """
        if self.ai_provider == "claude":
            self._initialize_claude()
        else:
            self._initialize_gemini()

    def _initialize_claude(self):
        """é…ç½® Anthropic Claude æ‘˜è¦æœå‹™

        ä½¿ç”¨ Claude API é‡‘é‘°å’ŒæŒ‡å®šæ¨¡å‹åˆå§‹åŒ–æ‘˜è¦å™¨ã€‚
        """
        if not AI_AVAILABLE:
            logger.warning("æœªå®‰è£ langchain_anthropic")
            return

        api_key = settings.CLAUDE_API_KEY
        if not api_key:
            logger.warning("æœªè¨­å®š CLAUDE_API_KEY")
            return

        try:
            self.llm = ChatAnthropic(
                model=self.model_name,
                temperature=0,
                max_tokens=self.max_tokens,
                anthropic_api_key=api_key,
            )
            logger.info("Claude æ‘˜è¦å™¨åˆå§‹åŒ–æˆåŠŸ")
            ai_logger.info(f"[CLAUDE] æ‘˜è¦å™¨åˆå§‹åŒ–æˆåŠŸ - æ¨¡å‹: {self.model_name}")
        except Exception as e:
            logger.error(f"Claude åˆå§‹åŒ–å¤±æ•—: {e}")
            self.llm = None

    def _initialize_gemini(self):
        """é…ç½® Google Gemini æ‘˜è¦æœå‹™

        ä½¿ç”¨ Gemini API é‡‘é‘°å’ŒæŒ‡å®šæ¨¡å‹åˆå§‹åŒ–æ‘˜è¦å™¨ã€‚
        """
        if not AI_AVAILABLE:
            logger.warning("æœªå®‰è£ langchain_google_genai")
            return

        api_key = settings.GEMINI_API_KEY
        if not api_key:
            logger.warning("æœªè¨­å®š GEMINI_API_KEY")
            return

        try:
            self.llm = ChatGoogleGenerativeAI(
                model=self.model_name, temperature=0, max_output_tokens=self.max_tokens
            )
            logger.info("Gemini æ‘˜è¦å™¨åˆå§‹åŒ–æˆåŠŸ")
            ai_logger.info(f"[GEMINI] æ‘˜è¦å™¨åˆå§‹åŒ–æˆåŠŸ - æ¨¡å‹: {self.model_name}")
        except Exception as e:
            logger.error(f"Gemini åˆå§‹åŒ–å¤±æ•—: {e}")
            self.llm = None

    def get_summary_prompt(self, command: str) -> str:
        """ç”¢ç”Ÿè¨­å‚™è¼¸å‡ºæ‘˜è¦çš„æç¤ºè©

        å»ºç«‹çµæ§‹åŒ–çš„æç¤ºï¼ŒæŒ‡å° AI ä¿ç•™é—œéµè¨Šæ¯ã€‚
        """
        return f"""è«‹æ‘˜è¦ä»¥ä¸‹ç¶²è·¯æŒ‡ä»¤è¼¸å‡ºï¼Œä¿ç•™é—œéµè¨ºæ–·è³‡è¨Šï¼š

æŒ‡ä»¤ï¼š{command}

è¦æ±‚ï¼š
1. ä¿ç•™éŒ¯èª¤ã€è­¦å‘Šã€ç•°å¸¸ç‹€æ…‹ã€é‡è¦æ•¸å€¼
2. æ’é™¤é‡è¤‡å…§å®¹å’Œç„¡é—œç´°ç¯€
3. ç¶­æŒæŠ€è¡“è¡“èªæº–ç¢ºæ€§
4. ä½¿ç”¨ç¹é«”ä¸­æ–‡
5. é–‹é ­æ¨™è¨»ï¼šã€Œ[AIæ‘˜è¦] åŸè¼¸å‡ºéé•·ï¼Œä»¥ä¸‹ç‚ºæ™ºèƒ½æ‘˜è¦ï¼šã€

è«‹ç›´æ¥è¼¸å‡ºæ‘˜è¦çµæœã€‚"""

    def summarize_output(self, command: str, output: str) -> str:
        """ä½¿ç”¨ AI æ‘˜è¦è¨­å‚™è¼¸å‡ºå…§å®¹

        å°‡è¶…é•·çš„è¨­å‚™è¼¸å‡ºé€é AI é€²è¡Œæ‘˜è¦ï¼Œä¿ç•™é—œéµè³‡è¨Šã€‚
        """
        if not self.llm:
            logger.warning(f"AI æ‘˜è¦å™¨ä¸å¯ç”¨: {command}")
            return self._fallback_truncate(command, output)

        try:
            logger.info(f"é–‹å§‹ AI æ‘˜è¦: {command}")
            ai_logger.info(
                f"[{self.ai_provider.upper()}] æ‘˜è¦é–‹å§‹ - æŒ‡ä»¤: {command}, é•·åº¦: {len(output)}"
            )

            prompt = self.get_summary_prompt(command)
            response = self.llm.invoke(f"{prompt}\n\nè¼¸å‡ºå…§å®¹ï¼š\n{output}")

            if response and hasattr(response, "content"):
                summary = response.content.strip()
                compression = round((1 - len(summary) / len(output)) * 100, 1)
                logger.info(f"æ‘˜è¦å®Œæˆ: {command}, å£“ç¸®ç‡: {compression}%")
                ai_logger.info(
                    f"[{self.ai_provider.upper()}] æ‘˜è¦å®Œæˆ - å£“ç¸®ç‡: {compression}%"
                )
                return summary

            logger.warning(f"AI æ‘˜è¦å¤±æ•—: {command}")
            return self._fallback_truncate(command, output)

        except Exception as e:
            logger.error(f"AI æ‘˜è¦éŒ¯èª¤: {e}")
            ai_logger.error(
                f"[{self.ai_provider.upper()}] æ‘˜è¦å¤±æ•— - {command}: {str(e)[:100]}"
            )
            return self._fallback_truncate(command, output)

    def _fallback_truncate(
        self, command: str, output: str, max_chars: int = 10000
    ) -> str:
        """ç•¶ AI æ‘˜è¦å¤±æ•—æ™‚çš„å¾Œå‚™è™•ç†

        ç°¡å–®åœ°æˆªæ–·è¼¸å‡ºå…§å®¹ä¸¦é™„ä¸Šè­¦å‘Šè¨Šæ¯ã€‚
        """
        return (
            output[:max_chars] + f"\n\n--- [è­¦å‘Š] æŒ‡ä»¤ '{command}' è¼¸å‡ºéé•·å·²æˆªæ–· ---"
        )


class AIService:
    """ä¸»è¦çš„ AI æ™ºèƒ½åˆ†ææœå‹™ç®¡ç†å™¨

    ç®¡ç† AI æœå‹™çš„åˆå§‹åŒ–ã€å·¥å…·é…ç½®å’ŒæŸ¥è©¢è™•ç†ï¼š
    - æ”¯æ´ Gemini å’Œ Claude é›™å¼•æ“è‡ªå‹•åˆ‡æ›
    - ReAct Agent æ¨¡å¼çš„æ€è€ƒéˆåˆ†æ
    - ç¶²è·¯è¨­å‚™æŒ‡ä»¤å·¥å…·ä»‹æ¥
    - çµæ§‹åŒ–è¼¸å‡ºå’Œæ ¼å¼åŒ–ç®¡ç†
    """

    def __init__(self):
        """åˆå§‹åŒ– AI æ™ºèƒ½åˆ†ææœå‹™

        è¨­å®š AI ä»£ç†ã€è¼¸å‡ºè§£æå™¨å’Œæç¤ºè©ç®¡ç†å™¨ã€‚
        """
        self.agent_executor = None
        self.ai_initialized = False

        # åˆå§‹åŒ– PydanticOutputParser
        self.parser = PydanticOutputParser(pydantic_object=NetworkAnalysisResponse)

        # åˆå§‹åŒ–æç¤ºè©ç®¡ç†å™¨
        self.prompt_manager = get_prompt_manager()

        # åˆå§‹åŒ– Token ä½¿ç”¨é‡è¿½è¹¤
        self.usage_callback = UsageMetadataCallbackHandler() if AI_AVAILABLE else None
        ai_logger.debug(
            f"UsageMetadataCallbackHandler åˆå§‹åŒ–: {self.usage_callback is not None}"
        )

        # åˆå§‹åŒ– AI ç³»çµ±
        self._initialize_ai()

    def _initialize_ai(self) -> bool:
        """åˆå§‹åŒ–å’Œé…ç½® AI æ™ºèƒ½åˆ†æå¼•æ“

        æª¢æŸ¥ API é‡‘é‘°å¯ç”¨æ€§ï¼Œåˆå§‹åŒ–é¸æ“‡çš„ AI æä¾›è€…ï¼Œ
        å»ºç«‹ ReAct ä»£ç†å’Œå·¥å…·é€£æ¥ã€‚

        Returns:
            bool: åˆå§‹åŒ–æ˜¯å¦æˆåŠŸ
        """
        if not AI_AVAILABLE:
            logger.warning("AI åŠŸèƒ½ä¸å¯ç”¨ï¼Œè·³éåˆå§‹åŒ–")
            return False

        try:
            # æª¢æŸ¥ç’°å¢ƒè®Šæ•¸è¼‰å…¥ç‹€æ…‹ (ä½¿ç”¨ Settings)
            google_api_key = settings.GEMINI_API_KEY
            anthropic_api_key = settings.CLAUDE_API_KEY
            ai_provider = settings.AI_PROVIDER

            # è¨˜éŒ„åˆå§‹åŒ–è³‡è¨Š
            debug_msg = f"AI åˆå§‹åŒ–é–‹å§‹ - æä¾›è€…: {ai_provider}"
            logger.info(debug_msg)

            if ai_provider == "gemini":
                if google_api_key:
                    logger.info("Google API Key å·²è¼‰å…¥")
                else:
                    error_msg = "Google API Key æœªè¨­å®šï¼Œç„¡æ³•åˆå§‹åŒ– Gemini"
                    logger.error(error_msg)
                    return False
            elif ai_provider == "claude":
                if anthropic_api_key:
                    logger.info("Anthropic API Key å·²è¼‰å…¥")
                else:
                    error_msg = "Anthropic API Key æœªè¨­å®šï¼Œç„¡æ³•åˆå§‹åŒ– Claude"
                    logger.error(error_msg)
                    return False

            # æ ¹æ“šæä¾›è€…åˆå§‹åŒ–å°æ‡‰çš„ LLM
            if ai_provider == "claude":
                llm = self._initialize_claude()
            else:
                llm = self._initialize_gemini()

            if llm is None:
                error_msg = f"{ai_provider.upper()} LLM åˆå§‹åŒ–å¤±æ•—"
                logger.error(error_msg)
                return False

            # å»ºç«‹å·¥å…·æ¸…å–®
            tools = self._create_tools()

            # å‰µå»ºåŒ…å«çµæ§‹åŒ–è¼¸å‡ºæ ¼å¼æŒ‡ä»¤çš„è‡ªå®šç¾©æç¤ºè©
            prompt_template = self._create_custom_prompt_template()
            agent = create_react_agent(llm, tools, prompt_template)
            self.agent_executor = AgentExecutor(
                agent=agent,
                tools=tools,
                verbose=False,
                handle_parsing_errors=True,
                return_intermediate_steps=True,  # å•Ÿç”¨ä¸­é–“æ­¥é©Ÿè¿”å›ä»¥ä¾¿æå– token ä½¿ç”¨é‡
            )

            # è¨˜éŒ„åˆå§‹åŒ–æˆåŠŸ
            init_success_message = (
                f"AI system initialized successfully (æä¾›è€…: {ai_provider.upper()})"
            )
            logger.info(init_success_message)

            # è¨˜éŒ„åˆ° AI å°ˆç”¨æ—¥èªŒ
            ai_logger.info(
                f"[{ai_provider.upper()}] AI ç³»çµ±åˆå§‹åŒ–æˆåŠŸ - æ¨¡å‹: {llm.__class__.__name__}"
            )

            self.ai_initialized = True
            return True

        except Exception as e:
            logger.error(f"AI system initialization failed: {e}")
            return False

    def _initialize_claude(self):
        """åˆå§‹åŒ– Claude AI"""
        api_key = settings.CLAUDE_API_KEY
        if not api_key:
            logger.warning("CLAUDE_API_KEY not set, Claude AI features unavailable")
            return None

        try:
            # å¾ Settings è®€å– Claude æ¨¡å‹
            claude_model = settings.CLAUDE_MODEL
            llm = ChatAnthropic(
                model=claude_model,
                temperature=0,
                anthropic_api_key=api_key,
                callbacks=[self.usage_callback] if self.usage_callback else [],
            )
            # è¨˜éŒ„åˆå§‹åŒ–è³‡è¨Š
            init_message = f"ä½¿ç”¨ Claude AI ä½œç‚ºä¸»è¦ AI æä¾›è€… - æ¨¡å‹: {claude_model}"
            logger.info(init_message)
            ai_logger.debug(
                f"Claude LLM åˆå§‹åŒ–å®Œæˆï¼Œcallback å·²é…ç½®: {self.usage_callback is not None}"
            )
            return llm
        except Exception as e:
            logger.error(f"Claude AI åˆå§‹åŒ–å¤±æ•—: {e}")
            return None

    def _initialize_gemini(self):
        """åˆå§‹åŒ– Gemini AI"""
        api_key = settings.GEMINI_API_KEY
        if not api_key:
            error_msg = "GEMINI_API_KEY æœªè¨­å®šï¼ŒGemini AI åŠŸèƒ½ä¸å¯ç”¨"
            logger.warning(error_msg)
            return None

        try:
            # å¾ Settings è®€å– Gemini æ¨¡å‹
            gemini_model = settings.GEMINI_MODEL

            # è¨˜éŒ„åˆå§‹åŒ–è³‡è¨Š
            init_start_msg = f"é–‹å§‹åˆå§‹åŒ– Gemini AI - æ¨¡å‹: {gemini_model}"
            logger.info(init_start_msg)

            llm = ChatGoogleGenerativeAI(
                model=gemini_model,
                temperature=0,
                google_api_key=api_key,
                callbacks=[self.usage_callback] if self.usage_callback else [],
            )

            # è¨˜éŒ„æˆåŠŸè¨Šæ¯
            success_msg = f"Gemini AI åˆå§‹åŒ–æˆåŠŸ - æ¨¡å‹: {gemini_model}"
            logger.info(success_msg)
            ai_logger.debug(
                f"Gemini LLM åˆå§‹åŒ–å®Œæˆï¼Œcallback å·²é…ç½®: {self.usage_callback is not None}"
            )
            return llm

        except Exception as e:
            error_msg = f"Gemini AI åˆå§‹åŒ–å¤±æ•—: {type(e).__name__}: {str(e)}"
            logger.error(error_msg)

            # è©³ç´°çš„éŒ¯èª¤è¨ºæ–·
            if "429" in str(e) or "quota" in str(e).lower():
                quota_msg = "å¯èƒ½æ˜¯ API é…é¡å·²ç”¨å®Œæˆ–è«‹æ±‚é »ç‡éé«˜"
                logger.error(quota_msg)
            elif "401" in str(e) or "unauthorized" in str(e).lower():
                auth_msg = "API Key å¯èƒ½ç„¡æ•ˆæˆ–æ¬Šé™ä¸è¶³"
                logger.error(auth_msg)
            elif "import" in str(e).lower() or "module" in str(e).lower():
                import_msg = (
                    "å¯èƒ½ç¼ºå°‘å¿…è¦çš„å¥—ä»¶ï¼Œè«‹æª¢æŸ¥ langchain-google-genai æ˜¯å¦æ­£ç¢ºå®‰è£"
                )
                logger.error(import_msg)

            return None

    def _create_tools(self) -> List[Tool]:
        """å»ºç«‹ AI å·¥å…·æ¸…å–®"""
        tools = [
            Tool(
                name="BatchCommandRunner",
                func=batch_command_wrapper,
                description="""
                ç¶²è·¯è¨­å‚™æŒ‡ä»¤åŸ·è¡Œå·¥å…· - è‡ªå‹•åŸ·è¡Œå®‰å…¨çš„ show é¡æŒ‡ä»¤ä¸¦è¿”å›çµæ§‹åŒ–çµæœã€‚

                **é—œéµé™åˆ¶**ï¼š
                - æ¯æ¬¡èª¿ç”¨åªèƒ½åŸ·è¡Œä¸€å€‹æŒ‡ä»¤
                - å¤šå€‹æŒ‡ä»¤å¿…é ˆåˆ†åˆ¥èª¿ç”¨æ­¤å·¥å…·å¤šæ¬¡
                - çµ•å°ç¦æ­¢åœ¨æ²’æœ‰åŸ·è¡Œçš„æƒ…æ³ä¸‹è™›æ§‹çµæœ

                **é‡è¦èªªæ˜**ï¼š
                - å·¥å…·æœƒè‡ªå‹•é©—è­‰æŒ‡ä»¤å®‰å…¨æ€§ï¼ŒåªåŸ·è¡Œå…è¨±çš„å”¯è®€æŒ‡ä»¤
                - æ”¯æ´æ‰€æœ‰æ¨™æº–çš„ show æŒ‡ä»¤ï¼ˆå¦‚ show version, show interface, show environment, show platform ç­‰ï¼‰
                - åŸ·è¡ŒæˆåŠŸæ™‚æœƒè¿”å›å®Œæ•´çš„è¨­å‚™è¼¸å‡ºè³‡æ–™
                - åŸ·è¡Œå¤±æ•—æ™‚æœƒæä¾›è©³ç´°çš„éŒ¯èª¤åˆ†æå’Œå»ºè­°

                **è¼¸å…¥æ ¼å¼**: 
                - "device_ip: command" (å–®ä¸€è¨­å‚™åŸ·è¡Œå–®ä¸€æŒ‡ä»¤)
                - "device_ip1,device_ip2: command" (å¤šè¨­å‚™åŸ·è¡ŒåŒä¸€æŒ‡ä»¤)
                - "command" (æ‰€æœ‰è¨­å‚™åŸ·è¡ŒåŒä¸€æŒ‡ä»¤)

                **å¤šæŒ‡ä»¤åŸ·è¡Œç¯„ä¾‹**:
                éŒ¯èª¤æ–¹å¼ï¼š
                - "202.153.183.18: show clock; show platform" âŒ (ä¸æ”¯æ´åˆ†è™Ÿåˆ†éš”)
                - "202.153.183.18: show clock and show platform" âŒ (ä¸æ”¯æ´ and é€£æ¥)

                æ­£ç¢ºæ–¹å¼ï¼š
                1. ç¬¬ä¸€æ¬¡èª¿ç”¨ï¼š"202.153.183.18: show clock"
                2. ç¬¬äºŒæ¬¡èª¿ç”¨ï¼š"202.153.183.18: show platform"
                3. ç¶œåˆå…©æ¬¡çµæœé€²è¡Œåˆ†æ

                **ä½¿ç”¨ä¾‹å­**: 
                - "203.0.113.20: show environment" (æŸ¥è©¢å–®ä¸€è¨­å‚™ç’°å¢ƒç‹€æ…‹)
                - "203.0.113.20,203.0.113.21: show version" (æŸ¥è©¢å¤šè¨­å‚™ç‰ˆæœ¬)
                - "show interface status" (æŸ¥è©¢æ‰€æœ‰è¨­å‚™ä»‹é¢ç‹€æ…‹)

                **åŸ·è¡Œçµæœæ ¼å¼**ï¼š
                å·¥å…·æœƒè¿”å›ä¸€å€‹ JSON æ ¼å¼çš„å­—ä¸²ï¼ŒåŒ…å«ä»¥ä¸‹çµæ§‹ï¼š
                {
                  "summary": {
                    "command": "åŸ·è¡Œçš„æŒ‡ä»¤",
                    "total_devices": ç¸½è¨­å‚™æ•¸,
                    "successful_devices": æˆåŠŸè¨­å‚™æ•¸,
                    "failed_devices": å¤±æ•—è¨­å‚™æ•¸,
                    "execution_time_seconds": åŸ·è¡Œæ™‚é–“(ç§’),
                    "cache_stats": {"hits": å¿«å–å‘½ä¸­æ¬¡æ•¸, "misses": å¿«å–æœªå‘½ä¸­æ¬¡æ•¸}
                  },
                  "successful_results": [
                    {"device_ip": "è¨­å‚™IP", "output": "è¨­å‚™è¼¸å‡ºå…§å®¹"}
                  ],
                  "failed_results": [
                    {
                      "device_ip": "è¨­å‚™IP",
                      "error_message": "éŒ¯èª¤è¨Šæ¯", 
                      "error_details": {
                        "type": "éŒ¯èª¤é¡å‹",
                        "category": "éŒ¯èª¤åˆ†é¡",
                        "suggestion": "è§£æ±ºå»ºè­°"
                      }
                    }
                  ]
                }

                **é‡è¦**ï¼šä½ å¿…é ˆè§£æé€™å€‹ JSON çµæœä¾†ç²å–è¨­å‚™çš„è¼¸å‡ºå’ŒéŒ¯èª¤è³‡è¨Šï¼Œç„¶å¾Œæä¾›å°ˆæ¥­çš„åˆ†æå’Œå»ºè­°ã€‚
                """,
            )
        ]

        return tools

    def _create_custom_prompt_template(self) -> PromptTemplate:
        """å»ºç«‹è‡ªå®šç¾©çš„ ReAct æç¤ºè©æ¨¡æ¿ï¼ŒåŒ…å« PydanticOutputParser æ ¼å¼æŒ‡ä»¤

        Returns:
            PromptTemplate: åŒ…å«çµæ§‹åŒ–è¼¸å‡ºæ ¼å¼çš„æç¤ºè©æ¨¡æ¿
        """
        # ç²å–æ ¼å¼æŒ‡ä»¤ä¸¦è™•ç†ç‰¹æ®Šå­—ç¬¦ï¼Œé¿å… LangChain è®Šæ•¸è¡çª
        format_instructions = self.parser.get_format_instructions()
        # å°‡ format_instructions ä¸­çš„èŠ±æ‹¬è™Ÿè½‰ç¾©ï¼Œé¿å…è¢«ç•¶ä½œ LangChain è®Šæ•¸
        escaped_format_instructions = format_instructions.replace("{", "{{").replace(
            "}", "}}"
        )

        # ä½¿ç”¨æç¤ºè©ç®¡ç†å™¨ç²å–ç³»çµ±æç¤ºè©
        base_prompt = self.prompt_manager.render_system_prompt(
            search_enabled=False,
            format_instructions=escaped_format_instructions,
        )

        # å° base_prompt ä¸­çš„èŠ±æ‹¬è™Ÿé€²è¡Œè½‰ç¾©ï¼Œé¿å… LangChain PromptTemplate å°‡å…¶è¦–ç‚ºè®Šæ•¸
        escaped_base_prompt = base_prompt.replace("{", "{{").replace("}", "}}")

        # å»ºç«‹ ReAct å·¥ä½œæµç¨‹æ¨¡æ¿
        template = f"""{escaped_base_prompt}

Answer the following questions as best you can. You have access to the following tools:

{{tools}}

Use the following format:

Question: the input question you must answer
Thought: you should always think about what to do
Action: the action to take, should be one of [{{tool_names}}]
Action Input: the input to the action
Observation: the result of the action
... (this Thought/Action/Action Input/Observation can repeat N times)
Thought: I now know the final answer
Final Answer: è«‹æŒ‰ç…§ä¸Šé¢ output_format éƒ¨åˆ†æŒ‡å®šçš„ JSON æ ¼å¼å›æ‡‰

Question: {{input}}
{{agent_scratchpad}}"""

        return PromptTemplate(
            template=template,
            input_variables=["input", "agent_scratchpad"],
            partial_variables={
                "tools": "\n".join(
                    [
                        f"{tool.name}: {tool.description}"
                        for tool in self._create_tools()
                    ]
                ),
                "tool_names": ", ".join([tool.name for tool in self._create_tools()]),
            },
        )

    async def query_ai(
        self,
        prompt: str,
        task_id: str,
        timeout: float = 120.0,
        include_examples: bool = True,
        device_ips: List[str] = None,
    ) -> str:
        """åŸ·è¡Œ AI æ™ºèƒ½åˆ†ææŸ¥è©¢

        ä½¿ç”¨ ReAct ä»£ç†æ¨¡å¼é€²è¡Œç¶²è·¯è¨­å‚™åˆ†æï¼Œç”¢ç”Ÿçµæ§‹åŒ–çš„
        Markdown æ ¼å¼åˆ†æå ±å‘Šã€‚

        Args:
            prompt: ç”¨æˆ¶çš„åˆ†ææŸ¥è©¢è«‹æ±‚
            timeout: æŸ¥è©¢è¶…æ™‚æ™‚é–“ï¼ˆç§’ï¼‰
            include_examples: æ˜¯å¦åŒ…å«æ€è€ƒéˆç¤ºç¯„ç¯„ä¾‹
            device_ips: é™åˆ¶åˆ†æçš„è¨­å‚™ IP ç¯„åœ

        Returns:
            str: Markdown æ ¼å¼çš„åˆ†æå ±å‘Š

        Raises:
            ExternalServiceError: AI æœå‹™ä¸å¯ç”¨æˆ–æŸ¥è©¢å¤±æ•—
        """
        if not self.ai_initialized or not self.agent_executor:
            raise ai_error("Gemini", "AI æœå‹™æœªå•Ÿç”¨æˆ–åˆå§‹åŒ–å¤±æ•—", "AI_NOT_AVAILABLE")

        # ä½¿ç”¨ PromptManager è™•ç†æç¤ºè©æ§‹å»º
        enhanced_prompt = self.prompt_manager.render_query_prompt(
            user_query=prompt,
            include_examples=include_examples,
            enable_guardrails=True,
            query_uuid=task_id,
            timestamp=time.time(),
            device_scope_restriction=device_ips,
        )

        # è¨­ç½®ç·šç¨‹æœ¬åœ°è¨­å‚™ç¯„åœé™åˆ¶ï¼ˆä¿ç•™åŸæœ‰æ©Ÿåˆ¶ï¼‰
        if device_ips:
            set_device_scope_restriction(device_ips)
        else:
            set_device_scope_restriction(None)

        try:
            # é‡ç½® callback ä»¥ä¾¿æ”¶é›†æ–°çš„ä½¿ç”¨é‡æ•¸æ“š
            if self.usage_callback:
                # æ¸…ç©ºç¾æœ‰çš„ä½¿ç”¨é‡æ•¸æ“šï¼Œè€Œä¸æ˜¯é‡æ–°å‰µå»º callback
                if hasattr(self.usage_callback, "usage_metadata"):
                    self.usage_callback.usage_metadata = {}
                ai_logger.debug("æ¸…ç©º UsageMetadataCallbackHandler æº–å‚™æ”¶é›†æ–°æ•¸æ“š")

            # åŸ·è¡Œ AI æŸ¥è©¢ - åŒæ™‚åœ¨ LLM å’Œ Agent å±¤ç´šè¨­ç½® callback
            config = {"callbacks": [self.usage_callback]} if self.usage_callback else {}
            ai_logger.debug(f"AgentExecutor é…ç½®: {config}")

            result = await asyncio.wait_for(
                asyncio.to_thread(
                    self.agent_executor.invoke, {"input": enhanced_prompt}, config
                ),
                timeout=timeout,
            )

            # è¨˜éŒ„åŸå§‹çµæœç”¨æ–¼èª¿è©¦
            ai_logger.debug(f"AI å›æ‡‰åŸå§‹çµæœé¡å‹: {type(result)}")
            if isinstance(result, dict):
                ai_logger.debug(f"AI å›æ‡‰éµå€¼: {list(result.keys())}")

            # å–å¾— Agent çš„æœ€çµ‚å›ç­”
            final_answer_str = (
                result.get("output", "") if isinstance(result, dict) else str(result)
            )

            if not final_answer_str.strip():
                raise ai_error("Gemini", "AI å›æ‡‰ç‚ºç©º", "AI_EMPTY_RESPONSE")

            try:
                # ä½¿ç”¨ PydanticOutputParser è§£æçµæ§‹åŒ–è¼¸å‡º
                structured_response: NetworkAnalysisResponse = self.parser.parse(
                    final_answer_str
                )

                # è¨˜éŒ„æˆåŠŸè§£æ
                ai_logger.info(
                    f"æˆåŠŸè§£æçµæ§‹åŒ–å›æ‡‰: {structured_response.analysis_type}"
                )

                # è¨˜éŒ„ Token ä½¿ç”¨é‡
                self._log_token_usage(task_id, result)

                # è½‰æ›ç‚º Markdown æ ¼å¼ä¸¦åŒ…å«æˆæœ¬è³‡è¨Šè¿”å›å‰ç«¯
                return self._create_query_result(
                    structured_response.to_markdown(), task_id, result
                )

            except Exception as parse_error:
                # å¦‚æœçµæ§‹åŒ–è§£æå¤±æ•—ï¼Œå˜—è©¦å¾Œå‚™è§£æ
                ai_logger.warning(f"PydanticOutputParser è§£æå¤±æ•—: {parse_error}")
                ai_logger.warning(f"åŸå§‹è¼¸å‡º: {final_answer_str[:500]}...")

                # å¾Œå‚™ç­–ç•¥ï¼šå˜—è©¦ç›´æ¥è¿”å›æ¸…ç†å¾Œçš„æ–‡æœ¬
                cleaned_response = final_answer_str.strip()

                # æ¸…ç† Final Answer æ¨™è¨˜
                if "Final Answer:" in cleaned_response:
                    cleaned_response = cleaned_response.split("Final Answer:")[
                        -1
                    ].strip()
                if "The final answer is" in cleaned_response:
                    cleaned_response = cleaned_response.replace(
                        "The final answer is", ""
                    ).strip()

                # æ¸…ç† markdown ä»£ç¢¼å¡Šæ¨™è¨˜
                if cleaned_response.startswith("```"):
                    # ç§»é™¤é–‹é ­çš„ ```json æˆ– ```
                    lines = cleaned_response.split("\n")
                    if lines[0].startswith("```"):
                        lines = lines[1:]  # ç§»é™¤ç¬¬ä¸€è¡Œ
                    if lines and lines[-1].strip() == "```":
                        lines = lines[:-1]  # ç§»é™¤æœ€å¾Œä¸€è¡Œ
                    cleaned_response = "\n".join(lines).strip()

                # å¦‚æœé‚„æœ‰æ®˜ç•™çš„ ``` æ¨™è¨˜ï¼Œå˜—è©¦ç›´æ¥ç§»é™¤
                cleaned_response = (
                    cleaned_response.replace("```json", "").replace("```", "").strip()
                )

                if cleaned_response:
                    ai_logger.info("ä½¿ç”¨å¾Œå‚™è§£æç­–ç•¥")
                    # è¨˜éŒ„ Token ä½¿ç”¨é‡
                    self._log_token_usage(task_id, result)
                    return self._create_query_result(cleaned_response, task_id, result)
                else:
                    raise Exception(f"çµæ§‹åŒ–è§£æå’Œå¾Œå‚™è§£æéƒ½å¤±æ•—: {parse_error}")

        except asyncio.TimeoutError:
            ai_logger.error(f"AI æŸ¥è©¢è¶…æ™‚: {timeout}ç§’")
            raise Exception(f"AI åˆ†æè™•ç†è¶…æ™‚ï¼ˆ{timeout}ç§’ï¼‰- è«‹ç¸®çŸ­æŸ¥è©¢å…§å®¹æˆ–ç¨å¾Œé‡è©¦")
        except Exception as e:
            error_str = str(e)
            ai_logger.error(f"AI æŸ¥è©¢åŸ·è¡Œå¤±æ•—: {error_str}")

            # æª¢æŸ¥æ˜¯å¦æ˜¯ API é…é¡éŒ¯èª¤
            if (
                "429" in error_str
                or "quota" in error_str.lower()
                or "rate limit" in error_str.lower()
            ):
                ai_logger.error("API é…é¡å·²ç”¨å®Œ")
                raise Exception(
                    "Google Gemini API å…è²»é¡åº¦å·²ç”¨å®Œï¼ˆ50æ¬¡/æ—¥ï¼‰ï¼Œè«‹ç­‰å¾…æ˜å¤©é‡ç½®æˆ–å‡ç´šä»˜è²»æ–¹æ¡ˆ"
                )
            elif "401" in error_str or "unauthorized" in error_str.lower():
                ai_logger.error("API èªè­‰å¤±æ•—")
                raise Exception("AI API èªè­‰å¤±æ•—ï¼Œè«‹æª¢æŸ¥ API Key è¨­å®š")
            else:
                raise Exception(f"AI æŸ¥è©¢åŸ·è¡Œå¤±æ•—: {error_str}")
        finally:
            # æ¸…é™¤ç·šç¨‹æœ¬åœ°è¨­å‚™ç¯„åœé™åˆ¶ï¼Œç¢ºä¿ä¸æœƒå½±éŸ¿å¾ŒçºŒæŸ¥è©¢
            set_device_scope_restriction(None)

    def _calculate_token_cost(
        self, provider: str, model: str, input_tokens: int, output_tokens: int
    ) -> float:
        """è¨ˆç®— Token ä½¿ç”¨æˆæœ¬

        Args:
            provider: AI æä¾›è€… (claude/gemini)
            model: æ¨¡å‹åç¨±
            input_tokens: è¼¸å…¥ token æ•¸é‡
            output_tokens: è¼¸å‡º token æ•¸é‡

        Returns:
            float: ä¼°ç®—æˆæœ¬ï¼ˆç¾å…ƒï¼‰
        """
        # Claude å®šåƒ¹ï¼ˆæ¯ 1,000 tokensï¼‰
        claude_pricing = {
            "claude-3-haiku-20240307": {"input": 0.00025, "output": 0.00125},
            "claude-3-5-haiku-20241022": {"input": 0.00025, "output": 0.00125},
            "claude-3-sonnet-20240229": {"input": 0.003, "output": 0.015},
            "claude-3-5-sonnet-20241022": {"input": 0.003, "output": 0.015},
            "claude-3-opus-20240229": {"input": 0.015, "output": 0.075},
        }

        # Gemini å®šåƒ¹ï¼ˆæ¯ 1,000 tokensï¼‰
        gemini_pricing = {
            "gemini-1.5-flash": {"input": 0.00015, "output": 0.0006},
            "gemini-1.5-pro": {"input": 0.00125, "output": 0.005},
            "gemini-pro": {"input": 0.00125, "output": 0.005},  # å‘å¾Œç›¸å®¹
        }

        try:
            if provider == "claude":
                pricing = claude_pricing.get(model)
                if not pricing:
                    # ä½¿ç”¨ Sonnet ä½œç‚ºé è¨­å®šåƒ¹
                    pricing = claude_pricing["claude-3-5-sonnet-20241022"]
            elif provider == "gemini":
                pricing = gemini_pricing.get(model)
                if not pricing:
                    # ä½¿ç”¨ Flash ä½œç‚ºé è¨­å®šåƒ¹
                    pricing = gemini_pricing["gemini-1.5-flash"]
            else:
                return 0.0

            # è¨ˆç®—æˆæœ¬ï¼š(tokens / 1000) * price_per_1000
            input_cost = (input_tokens / 1000.0) * pricing["input"]
            output_cost = (output_tokens / 1000.0) * pricing["output"]
            total_cost = input_cost + output_cost

            return round(total_cost, 6)  # ä¿ç•™ 6 ä½å°æ•¸

        except Exception as e:
            ai_logger.warning(f"æˆæœ¬è¨ˆç®—å¤±æ•—: {e}")
            return 0.0

    def _extract_token_usage(self, result: Any) -> Dict[str, int]:
        """å¾çµæœä¸­æå– Token ä½¿ç”¨é‡

        Args:
            result: AI æŸ¥è©¢çš„çµæœ

        Returns:
            Dict: åŒ…å« token ä½¿ç”¨é‡çš„å­—å…¸
        """
        usage_data = {}

        # æ–¹æ³•1: å¾ callback handler å–å¾—
        if self.usage_callback and hasattr(self.usage_callback, "usage_metadata"):
            callback_data = self.usage_callback.usage_metadata
            if callback_data:
                ai_logger.debug(f"å¾ callback å–å¾— token æ•¸æ“š: {callback_data}")

                # è™•ç†åµŒå¥—çš„ callback æ•¸æ“šçµæ§‹ (æŒ‰æ¨¡å‹åç¨±åˆ†çµ„çš„æ ¼å¼)
                if isinstance(callback_data, dict):
                    for key, value in callback_data.items():
                        if isinstance(value, dict) and "input_tokens" in value:
                            # é€™æ˜¯æŒ‰æ¨¡å‹åç¨±åµŒå¥—çš„çµæ§‹ï¼Œæå–å…§å±¤çš„ token æ•¸æ“š
                            ai_logger.debug(f"å¾æ¨¡å‹ {key} æå– token æ•¸æ“š: {value}")
                            usage_data.update(value)
                            break  # åªéœ€è¦ç¬¬ä¸€å€‹æ¨¡å‹çš„æ•¸æ“š
                    else:
                        # å¦‚æœæ²’æœ‰åµŒå¥—çµæ§‹ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹æ•¸æ“š
                        usage_data.update(callback_data)

        # æ–¹æ³•2: å¾ AgentExecutor çµæœçš„ intermediate_steps æå–
        if isinstance(result, dict) and "intermediate_steps" in result:
            ai_logger.debug(
                "æª¢æ¸¬åˆ° AgentExecutor çµæœï¼Œå˜—è©¦å¾ intermediate_steps æå– token è³‡è¨Š"
            )
            intermediate_steps = result["intermediate_steps"]
            accumulated_tokens = self._extract_from_intermediate_steps(
                intermediate_steps
            )
            if accumulated_tokens:
                ai_logger.debug(
                    f"å¾ intermediate_steps å–å¾—ç´¯ç© token æ•¸æ“š: {accumulated_tokens}"
                )
                usage_data.update(accumulated_tokens)

        # æ–¹æ³•3: å¾çµæœ metadata ä¸­æå–ï¼ˆé©ç”¨æ–¼ç›´æ¥ LLM èª¿ç”¨ï¼‰
        if isinstance(result, dict):
            # æª¢æŸ¥æ˜¯å¦æœ‰ usage_metadata
            if "usage_metadata" in result:
                metadata = result["usage_metadata"]
                ai_logger.debug(f"å¾çµæœ metadata å–å¾— token æ•¸æ“š: {metadata}")
                usage_data.update(metadata)

            # æª¢æŸ¥ Agent çµæœä¸­çš„ LLM èª¿ç”¨è¨˜éŒ„
            if "output" in result:
                output = result["output"]
                if hasattr(output, "usage_metadata"):
                    ai_logger.debug(
                        f"å¾ output usage_metadata å–å¾— token æ•¸æ“š: {output.usage_metadata}"
                    )
                    usage_data.update(output.usage_metadata)

        # æ–¹æ³•4: æª¢æŸ¥çµæœç‰©ä»¶çš„å±¬æ€§
        if hasattr(result, "usage_metadata") and result.usage_metadata:
            ai_logger.debug(f"å¾çµæœç‰©ä»¶å±¬æ€§å–å¾— token æ•¸æ“š: {result.usage_metadata}")
            usage_data.update(result.usage_metadata)

        # æ–¹æ³•5: å¾ Gemini response_metadata æå–ï¼ˆç‰¹æ®Šè™•ç†ï¼‰
        gemini_tokens = self._extract_gemini_token_usage(result)
        if gemini_tokens:
            ai_logger.debug(
                f"å¾ Gemini response_metadata å–å¾— token æ•¸æ“š: {gemini_tokens}"
            )
            usage_data.update(gemini_tokens)

        # æ–¹æ³•6: å¦‚æœæ˜¯ Gemini ä½†æ²’æœ‰åŸç”Ÿæ•¸æ“šï¼Œå‰‡ä½¿ç”¨æ–‡æœ¬ä¼°ç®—
        if settings.AI_PROVIDER == "gemini" and not usage_data:
            ai_logger.debug("Gemini ç„¡åŸç”Ÿ token æ•¸æ“šï¼Œå˜—è©¦æ–‡æœ¬ä¼°ç®—")
            estimated_tokens = self._extract_gemini_tokens_from_texts(result)
            if estimated_tokens:
                usage_data.update(estimated_tokens)
                ai_logger.debug(f"ä½¿ç”¨æ–‡æœ¬ä¼°ç®— Gemini tokens: {estimated_tokens}")

        # ç¢ºä¿ total_tokens è¨ˆç®—æ­£ç¢º
        if usage_data and "total_tokens" not in usage_data:
            input_tokens = usage_data.get("input_tokens", 0)
            output_tokens = usage_data.get("output_tokens", 0)
            if input_tokens > 0 or output_tokens > 0:
                usage_data["total_tokens"] = input_tokens + output_tokens

        return usage_data

    def _extract_from_intermediate_steps(
        self, intermediate_steps: List
    ) -> Dict[str, int]:
        """å¾ AgentExecutor çš„ intermediate_steps æå– Token ä½¿ç”¨é‡

        Args:
            intermediate_steps: AgentExecutor çš„ä¸­é–“åŸ·è¡Œæ­¥é©Ÿ

        Returns:
            Dict: ç´¯ç©çš„ token ä½¿ç”¨é‡
        """
        total_input_tokens = 0
        total_output_tokens = 0

        for step in intermediate_steps:
            try:
                if isinstance(step, tuple) and len(step) >= 2:
                    action, observation = step[0], step[1]

                    # æª¢æŸ¥ action æ˜¯å¦æœ‰ usage è³‡è¨Š
                    if hasattr(action, "usage_metadata") and action.usage_metadata:
                        usage = action.usage_metadata
                        total_input_tokens += usage.get("input_tokens", 0)
                        total_output_tokens += usage.get("output_tokens", 0)
                        ai_logger.debug(f"å¾ action æå– tokens: {usage}")

                    # æª¢æŸ¥ observation æ˜¯å¦æœ‰ usage è³‡è¨Š
                    if (
                        hasattr(observation, "usage_metadata")
                        and observation.usage_metadata
                    ):
                        usage = observation.usage_metadata
                        total_input_tokens += usage.get("input_tokens", 0)
                        total_output_tokens += usage.get("output_tokens", 0)
                        ai_logger.debug(f"å¾ observation æå– tokens: {usage}")

                    # æª¢æŸ¥ observation å­—ä¸²ä¸­æ˜¯å¦åŒ…å« usage è³‡è¨Š
                    if isinstance(observation, str) and "usage_metadata" in observation:
                        ai_logger.debug(
                            f"è§€å¯Ÿåˆ° observation åŒ…å« usage_metadata: {observation[:200]}"
                        )

            except Exception as e:
                ai_logger.debug(f"è™•ç† intermediate_step æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                continue

        if total_input_tokens > 0 or total_output_tokens > 0:
            return {
                "input_tokens": total_input_tokens,
                "output_tokens": total_output_tokens,
                "total_tokens": total_input_tokens + total_output_tokens,
            }
        return {}

    def _extract_gemini_token_usage(self, result: Any) -> Dict[str, int]:
        """å¾ Gemini ç‰¹å®šçš„ response_metadata æå– Token ä½¿ç”¨é‡

        Args:
            result: AI æŸ¥è©¢çµæœ

        Returns:
            Dict: Gemini token ä½¿ç”¨é‡
        """
        try:
            # æª¢æŸ¥æ˜¯å¦æ˜¯ Gemini æä¾›è€…
            if settings.AI_PROVIDER != "gemini":
                return {}

            # å˜—è©¦å¾ä¸åŒå±¤ç´šçš„ response_metadata æå–
            if isinstance(result, dict):
                # æª¢æŸ¥é ‚å±¤ response_metadata
                if "response_metadata" in result:
                    metadata = result["response_metadata"]
                    if isinstance(metadata, dict):
                        tokens = self._parse_gemini_metadata(metadata)
                        if tokens:
                            return tokens

                # æª¢æŸ¥ output çš„ response_metadata
                if "output" in result:
                    output = result["output"]
                    if hasattr(output, "response_metadata"):
                        tokens = self._parse_gemini_metadata(output.response_metadata)
                        if tokens:
                            return tokens

            # æª¢æŸ¥çµæœç‰©ä»¶çš„ response_metadata
            if hasattr(result, "response_metadata"):
                tokens = self._parse_gemini_metadata(result.response_metadata)
                if tokens:
                    return tokens

        except Exception as e:
            ai_logger.debug(f"æå– Gemini token ä½¿ç”¨é‡æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

        return {}

    def _parse_gemini_metadata(self, metadata: Dict) -> Dict[str, int]:
        """è§£æ Gemini çš„ metadata æ ¼å¼

        Args:
            metadata: Gemini çš„ response metadata

        Returns:
            Dict: è§£æå¾Œçš„ token ä½¿ç”¨é‡
        """
        try:
            # Gemini çš„ token ä½¿ç”¨é‡å¯èƒ½åœ¨ä¸åŒçš„éµä¸­
            possible_keys = [
                "usage_metadata",
                "token_count",
                "usage",
                "promptTokenCount",
                "candidatesTokenCount",
                "totalTokenCount",
            ]

            for key in possible_keys:
                if key in metadata:
                    usage_info = metadata[key]
                    ai_logger.debug(f"ç™¼ç¾ Gemini metadata ä¸­çš„ {key}: {usage_info}")

                    if isinstance(usage_info, dict):
                        # æ¨™æº–æ ¼å¼
                        if (
                            "input_tokens" in usage_info
                            and "output_tokens" in usage_info
                        ):
                            return usage_info

                        # Gemini ç‰¹å®šæ ¼å¼
                        input_tokens = usage_info.get(
                            "promptTokenCount", usage_info.get("prompt_token_count", 0)
                        )
                        output_tokens = usage_info.get(
                            "candidatesTokenCount",
                            usage_info.get("candidates_token_count", 0),
                        )
                        total_tokens = usage_info.get(
                            "totalTokenCount", usage_info.get("total_token_count", 0)
                        )

                        if input_tokens > 0 or output_tokens > 0:
                            return {
                                "input_tokens": input_tokens,
                                "output_tokens": output_tokens,
                                "total_tokens": total_tokens
                                or (input_tokens + output_tokens),
                            }

        except Exception as e:
            ai_logger.debug(f"è§£æ Gemini metadata æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

        return {}

    def _estimate_tokens(self, text: str) -> int:
        """ä¼°ç®—æ–‡æœ¬çš„ token æ•¸é‡

        åŸºæ–¼ç¶“é©—å…¬å¼ï¼š
        - è‹±æ–‡ï¼šç´„ 4 å­—ç¬¦ = 1 token
        - ä¸­æ–‡ï¼šç´„ 2 å­—ç¬¦ = 1 token
        - æ··åˆæ–‡æœ¬ï¼šä½¿ç”¨åŠ æ¬Šå¹³å‡

        Args:
            text: è¦ä¼°ç®—çš„æ–‡æœ¬

        Returns:
            int: ä¼°ç®—çš„ token æ•¸é‡
        """
        if not text:
            return 0

        # è¨ˆç®—å­—ç¬¦æ•¸
        char_count = len(text)

        # çµ±è¨ˆä¸­æ–‡å­—ç¬¦æ•¸é‡ (Unicode ç¯„åœ)
        chinese_chars = 0
        for char in text:
            if "\u4e00" <= char <= "\u9fff":  # ä¸­æ–‡å­—ç¬¦ç¯„åœ
                chinese_chars += 1

        # ä¼°ç®—é‚è¼¯
        english_chars = char_count - chinese_chars

        # ä¸­æ–‡ï¼š2 å­—ç¬¦ = 1 tokenï¼Œè‹±æ–‡ï¼š4 å­—ç¬¦ = 1 token
        estimated_tokens = (chinese_chars / 2.0) + (english_chars / 4.0)

        # è‡³å°‘è¿”å› 1 å€‹ tokenï¼ˆå¦‚æœæœ‰æ–‡æœ¬å…§å®¹ï¼‰
        return max(1, int(estimated_tokens)) if text.strip() else 0

    def _extract_token_usage_for_gemini(
        self, prompt: str, response: str
    ) -> Dict[str, any]:
        """ç‚º Gemini ä¼°ç®— token ä½¿ç”¨é‡

        Args:
            prompt: è¼¸å…¥æç¤ºæ–‡æœ¬
            response: è¼¸å‡ºå›æ‡‰æ–‡æœ¬

        Returns:
            Dict: åŒ…å«ä¼°ç®— token ä½¿ç”¨é‡çš„å­—å…¸ï¼Œæ¨™è¨˜ç‚ºä¼°ç®—å€¼
        """
        try:
            input_tokens = self._estimate_tokens(prompt)
            output_tokens = self._estimate_tokens(response)
            total_tokens = input_tokens + output_tokens

            return {
                "input_tokens": input_tokens,
                "output_tokens": output_tokens,
                "total_tokens": total_tokens,
                "estimated": True,  # æ¨™è¨˜ç‚ºä¼°ç®—å€¼
                "method": "char_based_estimation",
            }

        except Exception as e:
            ai_logger.debug(f"Gemini token ä¼°ç®—å¤±æ•—: {e}")
            return {}

    def _extract_gemini_tokens_from_texts(self, result: Any) -> Dict[str, int]:
        """å¾ AgentExecutor çµæœä¸­æå–æ–‡æœ¬ä¸¦ä¼°ç®— Gemini token

        Args:
            result: AgentExecutor çš„çµæœ

        Returns:
            Dict: ä¼°ç®—çš„ token ä½¿ç”¨é‡
        """
        try:
            if not isinstance(result, dict):
                return {}

            # æå–è¼¸å…¥æ–‡æœ¬
            input_text = result.get("input", "")

            # æå–è¼¸å‡ºæ–‡æœ¬
            output_text = result.get("output", "")
            if not output_text and hasattr(result.get("output"), "content"):
                output_text = result.get("output").content

            if input_text or output_text:
                estimated_usage = self._extract_token_usage_for_gemini(
                    input_text, output_text
                )
                if estimated_usage:
                    ai_logger.debug(
                        f"åŸºæ–¼æ–‡æœ¬ä¼°ç®— Gemini tokens: è¼¸å…¥={input_text[:50]}..., è¼¸å‡º={output_text[:50]}..."
                    )
                    return estimated_usage

        except Exception as e:
            ai_logger.debug(f"å¾æ–‡æœ¬ä¼°ç®— Gemini token æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

        return {}

    def _log_token_usage(self, task_id: str, result: Any = None):
        """è¨˜éŒ„ Token ä½¿ç”¨é‡åˆ°æ—¥èªŒ

        Args:
            task_id: ä»»å‹™çš„å”¯ä¸€è­˜åˆ¥ç¢¼
            result: AI æŸ¥è©¢çµæœï¼ˆå¯é¸ï¼Œç”¨æ–¼æå–é¡å¤–çš„ token è³‡è¨Šï¼‰
        """
        # å¾å¤šå€‹ä¾†æºæå– token ä½¿ç”¨é‡
        usage_data = self._extract_token_usage(result)

        ai_logger.debug(
            f"Token æ—¥èªŒæª¢æŸ¥ - callback å­˜åœ¨: {self.usage_callback is not None}"
        )
        ai_logger.debug(f"æå–åˆ°çš„ usage æ•¸æ“š: {usage_data}")

        if not usage_data:
            ai_logger.warning("ç„¡ Token ä½¿ç”¨é‡æ•¸æ“šå¯è¨˜éŒ„")
            return

        provider = settings.AI_PROVIDER
        model = settings.CLAUDE_MODEL if provider == "claude" else settings.GEMINI_MODEL
        input_tokens = usage_data.get("input_tokens", 0)
        output_tokens = usage_data.get("output_tokens", 0)
        total_tokens = usage_data.get("total_tokens", 0)

        ai_logger.debug(
            f"Token æ•¸æ“šæå– - è¼¸å…¥: {input_tokens}, è¼¸å‡º: {output_tokens}, ç¸½è¨ˆ: {total_tokens}"
        )

        # æª¢æŸ¥æ˜¯å¦ç‚ºä¼°ç®—å€¼
        is_estimated = usage_data.get("estimated", False)

        # è¨ˆç®—æˆæœ¬
        estimated_cost = self._calculate_token_cost(
            provider, model, input_tokens, output_tokens
        )

        # è¨˜éŒ„è©³ç´°çš„ Token ä½¿ç”¨é‡å’Œæˆæœ¬
        token_info = {
            "task_id": task_id,
            "provider": provider,
            "model": model,
            "input_tokens": input_tokens,
            "output_tokens": output_tokens,
            "total_tokens": total_tokens,
            "estimated_cost_usd": estimated_cost,
            "is_estimated": is_estimated,
            "estimation_method": usage_data.get("method", "") if is_estimated else None,
        }

        # æ ¼å¼åŒ– TOKEN_USAGE æ—¥èªŒä»¥ä¾¿æ–¼é–±è®€
        if is_estimated:
            # Gemini ä¼°ç®—å€¼çš„ç‰¹æ®Šæ ¼å¼
            formatted_usage = (
                f"Task: {task_id[:8]}... | "
                f"Provider: {provider.upper()} ({model}) | "
                f"Tokens: {input_tokens}â†’{output_tokens} (Total: {total_tokens}) ğŸ“Šä¼°ç®—å€¼ | "
                f"Cost: ~${estimated_cost:.6f} USD (ä¼°ç®—)"
            )
            # ä½¿ç”¨ç‰¹æ®Šçš„ä¼°ç®—æ—¥èªŒæ ¼å¼
            ai_logger.info(f"TOKEN_USAGE_ESTIMATED: {formatted_usage}")
        else:
            # æ­£å¸¸çš„ç²¾ç¢ºå€¼æ ¼å¼
            formatted_usage = (
                f"Task: {task_id[:8]}... | "
                f"Provider: {provider.upper()} ({model}) | "
                f"Tokens: {input_tokens}â†’{output_tokens} (Total: {total_tokens}) | "
                f"Cost: ${estimated_cost:.6f} USD"
            )
            ai_logger.info(f"TOKEN_USAGE: {formatted_usage}")

        # åŒæ™‚è¨˜éŒ„åŸå§‹ JSON æ ¼å¼ä¾› API è§£æä½¿ç”¨
        ai_logger.debug(f"TOKEN_USAGE_RAW: {token_info}")

        # å¯¦æ™‚æ§åˆ¶å°æ‘˜è¦é¡¯ç¤º
        self._display_token_summary(
            provider, total_tokens, estimated_cost, is_estimated
        )

    def _create_query_result(
        self, response_text: str, task_id: str, result: Any = None
    ) -> Dict[str, Any]:
        """å»ºç«‹åŒ…å«æˆæœ¬è³‡è¨Šçš„æŸ¥è©¢çµæœ

        Args:
            response_text: AI å›æ‡‰æ–‡æœ¬
            task_id: ä»»å‹™ ID
            result: åŸå§‹ AI çµæœï¼ˆç”¨æ–¼æå–æˆæœ¬è³‡è¨Šï¼‰

        Returns:
            Dict: åŒ…å« response å’Œ token_cost çš„çµæœ
        """
        # æå–æˆæœ¬è³‡è¨Š
        token_cost = None
        usage_data = self._extract_token_usage(result)

        if usage_data:
            provider = settings.AI_PROVIDER
            model = (
                settings.CLAUDE_MODEL if provider == "claude" else settings.GEMINI_MODEL
            )
            input_tokens = usage_data.get("input_tokens", 0)
            output_tokens = usage_data.get("output_tokens", 0)
            total_tokens = usage_data.get("total_tokens", 0)
            is_estimated = usage_data.get("estimated", False)
            estimated_cost = self._calculate_token_cost(
                provider, model, input_tokens, output_tokens
            )

            token_cost = {
                "input_tokens": input_tokens,
                "output_tokens": output_tokens,
                "total_tokens": total_tokens,
                "estimated_cost_usd": estimated_cost,
                "provider": provider,
                "model": model,
                "is_estimated": is_estimated,
            }

        return {"response": response_text, "token_cost": token_cost}

    def _display_token_summary(
        self, provider: str, total_tokens: int, cost: float, is_estimated: bool = False
    ):
        """åœ¨æ§åˆ¶å°é¡¯ç¤º Token ä½¿ç”¨é‡æ‘˜è¦

        Args:
            provider: AI æä¾›è€…
            total_tokens: ç¸½ Token æ•¸
            cost: ä¼°ç®—æˆæœ¬
            is_estimated: æ˜¯å¦ç‚ºä¼°ç®—å€¼
        """
        # æ ¹æ“šæ˜¯å¦ç‚ºä¼°ç®—å€¼é¸æ“‡ä¸åŒçš„é¡¯ç¤ºæ ¼å¼
        if is_estimated:
            if cost > 0.01:  # æˆæœ¬è¶…é 1 ç¾åˆ†æ™‚ç‰¹åˆ¥æ¨™è¨˜
                logger.warning(
                    f"ğŸ”¥ é«˜æˆæœ¬æŸ¥è©¢ - {provider.upper()}: ~{total_tokens} tokens (~${cost:.4f}) ğŸ“Šä¼°ç®—"
                )
            else:
                logger.info(
                    f"ğŸ“Š AI æŸ¥è©¢å®Œæˆ - {provider.upper()}: ~{total_tokens} tokens (~${cost:.4f}) ä¼°ç®—"
                )
        else:
            if cost > 0.01:  # æˆæœ¬è¶…é 1 ç¾åˆ†æ™‚ç‰¹åˆ¥æ¨™è¨˜
                logger.warning(
                    f"ğŸ”¥ é«˜æˆæœ¬æŸ¥è©¢ - {provider.upper()}: {total_tokens} tokens (${cost:.4f})"
                )
            else:
                logger.info(
                    f"ğŸ’¡ AI æŸ¥è©¢å®Œæˆ - {provider.upper()}: {total_tokens} tokens (${cost:.4f})"
                )

    def classify_ai_error(self, error_str: str) -> Tuple[str, int]:
        """åˆ†é¡ AI API éŒ¯èª¤ä¸¦è¿”å›éŒ¯èª¤è¨Šæ¯å’Œç‹€æ…‹ç¢¼

        Args:
            error_str: éŒ¯èª¤è¨Šæ¯å­—ä¸²

        Returns:
            (éŒ¯èª¤è¨Šæ¯, HTTPç‹€æ…‹ç¢¼)
        """
        ai_provider = settings.AI_PROVIDER

        error_lower = error_str.lower()

        # é…é¡å’Œé »ç‡é™åˆ¶éŒ¯èª¤ - æ“´å¤§æª¢æŸ¥ç¯„åœ
        if (
            "429" in error_str
            or "quota" in error_lower
            or "rate limit" in error_lower
            or "exceeded" in error_lower
            or "limit" in error_lower
            or "å·²ç”¨å®Œ" in error_str
            or "resource_exhausted" in error_lower
            or "usage_limit" in error_lower
        ):

            ai_logger.error(f"API é…é¡éŒ¯èª¤: {error_str}")

            if ai_provider == "claude":
                error_msg = "Claude API è«‹æ±‚é »ç‡é™åˆ¶ï¼Œè«‹ç¨å¾Œå†è©¦ï¼ˆå»ºè­°ç­‰å¾… 1-2 åˆ†é˜ï¼‰"
            else:
                error_msg = "Google Gemini API å…è²»é¡åº¦å·²ç”¨å®Œï¼ˆ50æ¬¡/æ—¥ï¼‰ï¼Œè«‹ç­‰å¾…æ˜å¤©é‡ç½®æˆ–å‡ç´šä»˜è²»æ–¹æ¡ˆã€‚æˆ–è€…å˜—è©¦ä½¿ç”¨ Claude AIã€‚"
            return error_msg, 429

        elif (
            "401" in error_str
            or "unauthorized" in error_str.lower()
            or "invalid api key" in error_str.lower()
        ):
            error_msg = f"{ai_provider.upper()} API èªè­‰å¤±æ•—ï¼Œè«‹æª¢æŸ¥ API Key è¨­å®š"
            return error_msg, 401

        elif "403" in error_str or "forbidden" in error_str.lower():
            error_msg = f"{ai_provider.upper()} API æ¬Šé™ä¸è¶³ï¼Œè«‹æª¢æŸ¥ API Key æ¬Šé™è¨­å®š"
            return error_msg, 403

        elif "500" in error_str or "internal server error" in error_str.lower():
            if ai_provider == "claude":
                error_msg = "Claude AI æœå‹™æš«æ™‚ä¸å¯ç”¨ï¼Œè«‹ç¨å¾Œå†è©¦"
            else:
                error_msg = "Google AI æœå‹™æš«æ™‚ä¸å¯ç”¨ï¼Œè«‹ç¨å¾Œå†è©¦"
            return error_msg, 502

        elif (
            "network" in error_str.lower()
            or "connection" in error_str.lower()
            or "timeout" in error_str.lower()
        ):
            error_msg = "ç¶²è·¯é€£æ¥å•é¡Œï¼Œè«‹æª¢æŸ¥ç¶²è·¯é€£æ¥å¾Œé‡è©¦"
            return error_msg, 503

        else:
            error_msg = f"{ai_provider.upper()} AI æŸ¥è©¢åŸ·è¡Œå¤±æ•—: {error_str}"
            return error_msg, 500

    def get_ai_status(self) -> Dict[str, Any]:
        """ç²å– AI æ™ºèƒ½åˆ†ææœå‹™çš„ç‹€æ…‹è³‡è¨Š

        æä¾› AI å¯ç”¨æ€§ã€åˆå§‹åŒ–ç‹€æ…‹å’Œé…ç½®è³‡è¨Šã€‚

        Returns:
            Dict: åŒ…å« AI æœå‹™ç‹€æ…‹çš„è©³ç´°è³‡è¨Š
        """
        ai_provider = settings.AI_PROVIDER

        # è¿”å› AI æœå‹™ç‹€æ…‹è³‡è¨Š
        return {
            "ai_available": AI_AVAILABLE,
            "ai_initialized": self.ai_initialized,
            "ai_provider": ai_provider,
            "pydantic_parser_enabled": True,
            "environment_config": {
                "PARSER_VERSION": settings.PARSER_VERSION,
            },
        }


# å…¨åŸŸ AI æœå‹™å¯¦ä¾‹
_ai_service = None


def get_ai_service() -> AIService:
    """ç²å–å…¨åŸŸ AI æ™ºèƒ½åˆ†ææœå‹™å¯¦ä¾‹

    å–®ä¾‹æ¨¡å¼çš„ AI æœå‹™ï¼Œç¢ºä¿åœ¨æ‡‰ç”¨ç¨‹å¼ä¸­
    ä½¿ç”¨ç›¸åŒçš„ AI é…ç½®å’Œç‹€æ…‹ã€‚
    """
    global _ai_service
    if _ai_service is None:
        logger.info("å»ºç«‹ AI æœå‹™å¯¦ä¾‹")
        _ai_service = AIService()
    return _ai_service


# AI æ‘˜è¦æœå‹™å¯¦ä¾‹ï¼ˆç”¨æ–¼è¶…é•·è¼¸å‡ºè™•ç†ï¼‰
output_summarizer = OutputSummarizer()
